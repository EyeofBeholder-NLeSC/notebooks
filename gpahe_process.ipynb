{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7beda2b5-3c6b-43b7-9465-7bc753976b30",
   "metadata": {},
   "source": [
    "# Process GPAHE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0631237a-407e-424c-8173-66ab2922ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from IPython.display import Image\n",
    "from wikidata.client import Client\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c37271-6d39-47cf-bb8a-5febbfad283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"../../data/scraped_symbol_dict.json\"\n",
    "data_df = pd.read_json(DATA_FILE).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868c8491-144f-4a9a-8030-7e9632efa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: \n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9b582-5c8e-428f-9ec3-1c84a0aeebf3",
   "metadata": {},
   "source": [
    "## 1. Cluster hate symbols\n",
    "\n",
    "Use https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68bd9c4-8afb-4179-a798-0cc455175801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import regex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a9d7c-215b-4ae7-a49a-ceb3e776e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_data(data_df):\n",
    "    texts = []\n",
    "    for _, row in data_df.iterrows():\n",
    "        description = row[\"Description\"]\n",
    "        ideology = \". ideology is \" + regex.sub(\",\", \" . ideology is\", row[\"Ideology\"])\n",
    "        location = \". location is \" + regex.sub(\",\", \" . location is\", row[\"Location\"])\n",
    "        texts.append(\" \".join(word_tokenize(\" \".join([description, ideology, location]))))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c55a1-2b12-41b9-90f0-c3c5dfb5992f",
   "metadata": {},
   "source": [
    "Interesting words for clustering:\n",
    "* chapter\n",
    "* club/klub\n",
    "* group\n",
    "* organization\n",
    "* party\n",
    "* proud (boys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6067fc7-f7a9-4895-b3ad-d8a41fb28e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text_data(texts, nbr_of_dimensions=5):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    vectorized_data = TruncatedSVD(n_components=nbr_of_dimensions, n_iter=5, random_state=42)\n",
    "    vectorized_data.fit(X.T)\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9636a-cdc3-4245-9046-623df8a415c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(texts, vectorized_data, labels, dimension_1=0, dimension_2=1):\n",
    "    try:\n",
    "        x = vectorized_data.components_[dimension_1]\n",
    "        y = vectorized_data.components_[dimension_2]\n",
    "    except:\n",
    "        raise ValueError(f\"invalid pair of dimensions ({dimension_1}, {dimension_2})\")\n",
    "    split_data, labels = split_data_by_content(x, y, texts, labels)\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    for color in sorted(split_data.keys(), reverse=True):\n",
    "        plt.scatter(split_data[color][0], split_data[color][1], c=color, label=color, alpha=0.5)\n",
    "    for i in range(0, len(x)):\n",
    "        plt.annotate(str(i), (x[i], y[i]))\n",
    "    plt.legend(labels=[ label for color, label in sorted(labels.items(), reverse=True) ] )\n",
    "    plt.title(f\"{len(x)} hate symbols clustered by description, ideology and location\")\n",
    "    plt.savefig(\"gpahe_process.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf9523-88e5-4309-8fa8-0dd1cbff8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_token = \"proud boys\"\n",
    "orange_token = \"chapter\"\n",
    "yellow_token = \"club\"\n",
    "green_token = \"group\"\n",
    "other_color = \"blue\"\n",
    "labels = { \"yellow\": yellow_token, \"red\": red_token, \"orange\": orange_token, \"green\": green_token, other_color: \"other\" }\n",
    "\n",
    "def split_data_by_content(x, y, texts, labels):\n",
    "    split_data = { other_color: [[], []], \"green\": [[], []], \"orange\": [[], []], \"red\": [[], []], \"yellow\": [[], []] }\n",
    "    for i in range(0, len(x)):\n",
    "        if regex.search(red_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"red\"][0].append(x[i])\n",
    "            split_data[\"red\"][1].append(y[i])\n",
    "        elif regex.search(orange_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"orange\"][0].append(x[i])\n",
    "            split_data[\"orange\"][1].append(y[i])\n",
    "        elif regex.search(yellow_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"yellow\"][0].append(x[i])\n",
    "            split_data[\"yellow\"][1].append(y[i])\n",
    "        elif regex.search(green_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"green\"][0].append(x[i])\n",
    "            split_data[\"green\"][1].append(y[i])\n",
    "        else:\n",
    "            split_data[other_color][0].append(x[i])\n",
    "            split_data[other_color][1].append(y[i])\n",
    "    for color in labels:\n",
    "        labels[color] += f\" ({len(split_data[color][0])})\"\n",
    "    return split_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8a54c-e85d-429e-9957-3813e363a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_text_data(data_df)\n",
    "vectorized_data = vectorize_text_data(texts)\n",
    "visualize(texts, vectorized_data, labels.copy(), dimension_1=1, dimension_2=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4efd3d-b836-4500-9502-6be3bdb9d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.iloc[422][\"Description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975b3cd-85b7-4504-8b7c-b4b189a12fdc",
   "metadata": {},
   "source": [
    "## 2. Make a knowledge base\n",
    "\n",
    "* https://medium.com/nlplanet/building-a-knowledge-base-from-texts-a-full-practical-example-8dbbffb912fa\n",
    "* https://neo4j.com/blog/text-to-knowledge-graph-information-extraction-pipeline/\n",
    "* https://neo4j.com/developer-blog/construct-knowledge-graphs-unstructured-text/\n",
    "* https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a449e-e629-4ee8-b02c-c6b2d1a67cab",
   "metadata": {},
   "source": [
    "## 3. Extract entities with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fd6be-43fc-4f22-865f-ebeb26c42ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_FIELD_NAMES = \"CARDINAL DATE EVENT FAC GPE LANGUAGE LAW LOC MONEY NOMINAL NORP ORDINAL ORG PERCENT PERSON PRODUCT QUANTITY TIME WORK_OF_ART\".split()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba3d19-355f-436c-bd86-a1f929a68f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_values = { \"organizations\": [\"NORP\", \"ORG\" ],\n",
    "                 \"locations\": [ \"FAC\", \"GPE\", \"LOC\", ], \n",
    "                 \"events\": [ \"EVENT\", ],\n",
    "                 \"dates\": [ \"DATE\", \"TIME\", ], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6dbe2a-0378-47c1-adc8-9f0a41698aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(nlp_analysis):\n",
    "    entities = {}\n",
    "    for entity in nlp_analysis.ents:\n",
    "        if entity.label_ not in entities:\n",
    "            entities[entity.label_] = []\n",
    "        entities[entity.label_].append(entity.text)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15c641-f955-4118-ad8d-8dd5c350ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_analysis(texts, nlp):\n",
    "    entity_data = []\n",
    "    for text in texts:\n",
    "        nlp_analysis = nlp(text) \n",
    "        entities = get_entities(nlp_analysis)\n",
    "        entity_data.append(entities)\n",
    "    return entity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b2e82-d2ca-4077-a248-af43ac9a3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entity_groups(entity_data, entity_group_name, n=10):\n",
    "    values = {}\n",
    "    for entities in entity_data:\n",
    "        for entity_name in entities:\n",
    "            if entity_name in field_values[entity_group_name]:\n",
    "                for entity in entities[entity_name]:\n",
    "                    if entity in values:\n",
    "                        values[entity] += 1\n",
    "                    else:\n",
    "                        values[entity] = 1\n",
    "    print(f\"{entity_group_name} ({sum(values.values())}):\", [[key, value] for key, value in sorted(values.items(), key=lambda item: item[1], reverse=True)][:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5b9b4-c52e-4c2b-9463-9f83bf5575cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entities(entity_data, target_entity_name, n=10):\n",
    "    values = {}\n",
    "    for entities in entity_data:\n",
    "        for entity_name in entities:\n",
    "            if entity_name == target_entity_name:\n",
    "                for entity in entities[entity_name]:\n",
    "                    if entity in values:\n",
    "                        values[entity] += 1\n",
    "                    else:\n",
    "                        values[entity] = 1\n",
    "    print(f\"{target_entity_name} ({sum(values.values())}):\", [[key, value] for key, value in sorted(values.items(), key=lambda item: item[1], reverse=True)][:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bf4d1-1c22-456c-b63e-fbc041a8f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data = nlp_analysis(texts, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dddc96-bf7c-4016-b316-7eed2ee8d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field_name in SPACY_FIELD_NAMES:\n",
    "    count_entities(entity_data, field_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35f45c-f39e-4a49-adde-43ccfcf469a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"organizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcd6fc-8f02-4571-90e7-1301ab080712",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de616c-e236-4aca-879d-c63636ea780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e35ad4-dc31-4cd3-8a6d-56e9882b55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31707462-0f3d-40e1-b159-74dc18e11611",
   "metadata": {},
   "source": [
    "## 4. Extract entities with REBEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90fa06-3867-47a9-98b3-28ef5c9bdf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "import wikipedia\n",
    "from newspaper import Article, ArticleException\n",
    "from GoogleNews import GoogleNews\n",
    "import IPython\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe418b8-b76c-4517-a636-4becbefa0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976491bc-7807-489c-ba1c-21940fd8f965",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# https://gist.githubusercontent.com/fabiochiusano/934ad5ff318626befbdd20c72e074186/raw/e3e44110a0db5408d17fba52be559ecaf676b6d2/kb_4.py\n",
    "\n",
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        relations.append({\n",
    "            'head': subject.strip(),\n",
    "            'type': relation.strip(),\n",
    "            'tail': object_.strip()\n",
    "        })\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa85990-f73e-44bc-a7d4-31d1d83e07d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# https://gist.githubusercontent.com/fabiochiusano/e64d5250371e18f7a6cc02ac0cdc64c5/raw/24af0f7f23b313591fe91fc9f8826cf216ca4568/kb_5.py\n",
    "\n",
    "class KB():\n",
    "    def __init__(self):\n",
    "        self.relations = []\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "    def add_relation(self, r):\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4edd6d-e03c-494f-992f-b5b5288f5764",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# https://gist.githubusercontent.com/fabiochiusano/ceec4d9ff1ce2ad25c40fbd8412aa9e4/raw/796771f88776fca9d7c4c84bd1b3a52d9ef5b5c1/kb_6.py\n",
    "\n",
    "def from_small_text_to_kb(text, verbose=False):\n",
    "    kb = KB()\n",
    "\n",
    "    # Tokenizer text\n",
    "    model_inputs = tokenizer(text, max_length=512, padding=True, truncation=True,\n",
    "                            return_tensors='pt')\n",
    "    if verbose:\n",
    "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
    "\n",
    "    # Generate\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 216,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": 3\n",
    "    }\n",
    "    generated_tokens = model.generate(\n",
    "        **model_inputs,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "    # create kb\n",
    "    for sentence_pred in decoded_preds:\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for r in relations:\n",
    "            kb.add_relation(r)\n",
    "\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de33da-7d9c-41e1-a0dc-cdc60890cc76",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# https://gist.githubusercontent.com/fabiochiusano/a720da218ee8d19de3130fa36c23a69b/raw/a9b94a3ddbad61cfb3713234476423fffbfdca41/kb_7.py\n",
    "\n",
    "def extract_relations_per_text(texts):\n",
    "    relations_per_text = []\n",
    "    for text in texts:\n",
    "        relations = []\n",
    "        for sentence in sent_tokenize(text):\n",
    "            kb = from_small_text_to_kb(sentence, verbose=True)\n",
    "            relations.extend(kb.__dict__[\"relations\"])\n",
    "        relations_per_text.append(relations)\n",
    "        squeal(f\"{len(relations_per_text)}: {sum([len(relations) for relations in relations_per_text])/len(relations_per_text)}\")\n",
    "    return relations_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61952a3-931c-44ee-ad64-d67f7eb4e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_relations(relations_per_text):\n",
    "    relations_count = {}\n",
    "    for relations in relations_per_text:\n",
    "        for relation in relations:\n",
    "            key = \"#\".join([relation[\"head\"], relation[\"type\"], relation[\"tail\"]])\n",
    "            if key in relations_count:\n",
    "                relations_count[key] += 1\n",
    "            else:\n",
    "                relations_count[key] = 1\n",
    "    return relations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc33865-41da-45fb-9400-28384d861fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relations_to_df(relations_per_text):\n",
    "    relations_list = []\n",
    "    for counter in range(0, len(relations_per_text)):\n",
    "        for relation in relations_per_text[counter]:\n",
    "            relation[\"text_id\"] = counter\n",
    "            relations_list.append(relation)\n",
    "    relations_df = pd.DataFrame(relations_list)\n",
    "    return relations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8237c-c974-444f-84c3-35c5fe14a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_of_lists(dict_of_lists):\n",
    "    return [ (len(list), key) for key, list in sorted(dict_of_lists.items(), key=lambda x: len(x[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd6932-9815-4421-88a9-ea327df2ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicates(relations_df):\n",
    "    nbr_of_duplicates = 0\n",
    "    seen = {}\n",
    "    for index, relation in relations_df.iterrows():\n",
    "        key = \"#\".join([relation[\"head\"], relation[\"type\"], relation[\"type\"]])\n",
    "        if key in seen:\n",
    "            nbr_of_duplicates += 1\n",
    "        seen[key] = True\n",
    "    return nbr_of_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a8205-1dc3-46b1-afb2-cc8c243a6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_relation_fields(relations_df):\n",
    "    head_terms = {}\n",
    "    tail_terms = {}\n",
    "    type_head_terms = {}\n",
    "    type_tail_terms = {}\n",
    "\n",
    "    for index, relation in relations_df.iterrows():\n",
    "        head = relation[\"head\"]\n",
    "        tail = relation[\"tail\"]\n",
    "        type = relation[\"type\"]\n",
    "        if head not in head_terms:\n",
    "            head_terms[head] = []\n",
    "        if tail not in head_terms[head]:\n",
    "            head_terms[head].append(tail)\n",
    "        if tail not in tail_terms:\n",
    "            tail_terms[tail] = []\n",
    "        if head not in tail_terms[tail]:\n",
    "            tail_terms[tail].append(head)\n",
    "        key = f\"{type}({head},_)\"\n",
    "        if key not in type_head_terms:\n",
    "            type_head_terms[key] = []\n",
    "        if tail not in type_head_terms[key]:\n",
    "            type_head_terms[key].append(tail)\n",
    "        key = f\"{type}(_,{tail})\"\n",
    "        if key not in type_tail_terms:\n",
    "            type_tail_terms[key] = []\n",
    "        if head not in type_tail_terms[key]:\n",
    "            type_tail_terms[key].append(head)\n",
    "    return head_terms, tail_terms, type_head_terms, type_tail_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98fbe3-b1d9-48d4-878b-190d2106103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_per_text = extract_relations_per_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0751015-edda-4152-8b67-ffbe03b5e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_count = count_relations(relations_per_text)\n",
    "print(f\"number of relations: {sum(relations_count.values())}; unique: {len(relations_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5f101-2f5f-4fd2-8110-9780c77a059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_freqs(relations_count)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe522989-9689-4af2-9d5a-805c3efb8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df = convert_relations_to_df(relations_per_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffcda8-8bab-459f-bfa8-c1c5307c1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df.to_csv(\"gpahe_process.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee6172-a257-4967-84a5-2dfa8d0a6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_terms, tail_terms, type_head_terms, type_tail_terms = count_relation_fields(relations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583abde0-22c5-454e-8bf9-cc56a5cee7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(head_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef68304-e25f-471a-bd36-5e35b05720f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(tail_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0abac8-f7d6-4ea7-a1fe-1751b0d6c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(type_head_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f5d5f-3fe5-4d68-ab2b-d6eb86f192eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(type_tail_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5b912-fa54-42a1-a5ea-5c6eb2dbd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f210e0-f9b4-486c-804b-4230c4172e27",
   "metadata": {},
   "source": [
    "## 5. Visualize knowlegde triples\n",
    "\n",
    "To do: add labels to edges (note: an edge can have several labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda225b-09cc-4dec-a436-17f6f74b4b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec5459-7768-45dd-af33-a8509804465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(relations_df, source=\"head\", target=\"tail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d803385-77d0-4d91-9424-7564bcbe812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(notebook=True)\n",
    "net.from_nx(G)\n",
    "net.show(\"gpahe_process.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606e82b-0198-48d0-a33d-5c90f2c33a24",
   "metadata": {},
   "source": [
    "## 6. Link to wikidata (fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787b9b8-8346-4e20-9483-9b028bba6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()  # doctest: +SKIP\n",
    "entity = client.get('Q20145', load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eec733-ee86-4966-b77c-fbd2fe1c9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f79870-0780-41f6-9ba1-6f35dc4214ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "from pywikibot import pagegenerators, WikidataBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b10966-6ae2-410e-9755-a55dbc6e742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = \"SELECT ?item WHERE { ?item rdfs:label 'Google'@en }\"\n",
    "entities = pagegenerators.WikidataSPARQLPageGenerator(sparql)\n",
    "entities = list(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa72825-6b52-486e-8879-e269ad83e7ff",
   "metadata": {},
   "source": [
    "## 7. download logo's manually (lot of work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773f3e21-55f6-44e3-adeb-638b669f0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2ffa22-a21a-45e7-addb-751c8171a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://symbols.globalextremism.org/details?recordId=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0b558a6-8dd1-4e6d-a2b3-4f641c712899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9560/1927802637.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m        \u001b[0mwebbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m        \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/eye/venv3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStdinNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1262\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/eye/venv3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "COUNTER_MINIMUM = 80\n",
    "counter = 0\n",
    "for id in data_df.index:\n",
    "    counter += 1\n",
    "    if counter > COUNTER_MINIMUM:\n",
    "       webbrowser.open(base_url + id, new=2)\n",
    "       print(counter)\n",
    "       input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3467e9-0196-46ce-84bc-2d54e2953ee4",
   "metadata": {},
   "source": [
    "1. open the web page in a new tab of a browser\n",
    "2. right click on the logo/image\n",
    "3. open the image in a new tab of the browser\n",
    "4. right click on the image\n",
    "5. save the image with name number.extension\n",
    "6. close the two added tabs\n",
    "7. push the enter/return button on the notebook page\n",
    "8. repeat for the next logo/image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5e1e48d-94ac-473c-a067-a1ff49a0d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = os.environ[\"HOME\"] + \"/Downloads\"\n",
    "download_files = os.listdir(download_dir)\n",
    "extensions = [ \"jpg\", \"png\", \"svg\", \"webp\", \"JPG\", \"PNG\", \"SVG\", \"WEBP\" ]  \n",
    "counter = 0\n",
    "for id in data_df.index:\n",
    "    counter += 1\n",
    "    if counter > COUNTER_MINIMUM:\n",
    "        break\n",
    "    file_found = False\n",
    "    for extension in extensions:\n",
    "        if os.path.isfile(download_dir + \"/\" + str(counter) + \".\" + extension):\n",
    "            file_found = True\n",
    "            break\n",
    "    if not file_found:\n",
    "        print(f\"cannot find file number {counter}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d243e-23ae-4e53-935a-69502938068f",
   "metadata": {},
   "source": [
    "## 8. Link Spacy entities from ChatGPT output to GPAHE metedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c85de4-b840-4cbd-b6be-c15e71af29c0",
   "metadata": {},
   "source": [
    "From the Spacy analysis we select all:\n",
    "1. noun phrases\n",
    "2. entities\n",
    "3. tokens with pos tag PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "76c13708-2209-4376-8019-f480b2ad18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_chars = str.maketrans(\"*#\", \"  \")\n",
    "\n",
    "def get_phrases(text, model):\n",
    "    nlp_analysis = nlp(text.translate(remove_chars))\n",
    "    chunk_texts = [ regex.sub(\"^[Tt][Hh][EeIi][Ss]* \", \"\", \n",
    "                        regex.sub(\"^[Aa][Nn]* \", \"\", chunk.text, \n",
    "                                  regex.IGNORECASE), \n",
    "                              regex.IGNORECASE) \n",
    "                     for chunk in nlp_analysis.noun_chunks\n",
    "                  ]\n",
    "    chunk_texts.extend([entity.text for entity in nlp_analysis.ents])\n",
    "    chunk_texts.extend([token.text for token in nlp_analysis if token.pos_ == \"PROPN\" ])\n",
    "    return chunk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3a4aced-22ea-423c-b5c6-c9e86998f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_list(data_df):\n",
    "    term_dict = {}\n",
    "    for index, row in data_df.iterrows():\n",
    "        term_dict[row[\"Title\"]] = True\n",
    "        for ideology in row[\"Ideology\"].split(\",\"):\n",
    "            term_dict[ideology.strip()] = True\n",
    "        for location in row[\"Location\"].split(\",\"):\n",
    "            term_dict[location.strip()] = True\n",
    "    return sorted(term_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "dee8f1d6-58b1-4b3c-884d-798561d90133",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_dir = \"chatgpt\"\n",
    "\n",
    "def read_chatgpt_texts(chatgpt_dir):\n",
    "    chatgpt_files = [ file_name for file_name in os.listdir(chatgpt_dir)\n",
    "                      if regex.search(\"b.txt\", file_name) ]\n",
    "    chatgpt_texts = {}\n",
    "    for file_name in sorted(chatgpt_files):\n",
    "        file_handle = open(os.path.join(chatgpt_dir, file_name), \"r\")\n",
    "        lines = file_handle.readlines()\n",
    "        file_handle.close()\n",
    "        chatgpt_texts[\"_\".join([chatgpt_dir, file_name])] = \" \".join(lines)\n",
    "    return chatgpt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f0257e06-3b44-4cac-a127-3635c792caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "term_list = get_term_list(data_df)\n",
    "chatgpt_texts = read_chatgpt_texts(chatgpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "51d0e06e-4f79-4aa8-87a4-37c101ab0a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4/20']\n",
      "['neo-Nazi']\n",
      "['Pepe the Frog']\n",
      "['Germany', 'Nazi']\n"
     ]
    }
   ],
   "source": [
    "term_list_lower = [ term.lower() for term in term_list ]\n",
    "meme_texts = {}\n",
    "for file_name in sorted(chatgpt_texts.keys()):\n",
    "    phrases = list(set(get_phrases(chatgpt_texts[file_name], spacy_model)))\n",
    "    phrases_in_term_list = [ phrase for phrase in phrases if phrase.lower() in term_list_lower ]\n",
    "    meme_texts[\"_\".join([chatgpt_dir, file_name])] = get_meme_text_from_chatgpt_text(chatgpt_texts[file_name])\n",
    "    print(phrases_in_term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9251296f-8374-4db0-9096-5af061db1af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 1. Interpretation of the Image\\n The image shows a dog with a happy, slightly mischievous expression. The background is decorated with colorful, psychedelic patterns that resemble marijuana leaves, often associated with a state of altered consciousness or celebration.\\n \\n ### 2. Interpretation of the Text\\n The text reads: \"ITS GONNA BE 4/20 FOR A WHOLE MONTH.\" This is a play on the date April 20th (4/20), which is widely recognized in cannabis culture as a day for celebrating and consuming marijuana.\\n \\n ### 3. Interpretation of the Combination\\n The combination of the happy, relaxed dog and the text implies a humorous and exaggerated scenario where the state of celebration and relaxation associated with 4/20 lasts for an entire month. The dog\\'s expression, along with the colorful background, reinforces the playful and light-hearted tone of the message, suggesting an extended period of enjoyment and leisure.\\n \\n In essence, the meme is using humor to exaggerate the idea of an extended celebration of cannabis culture, combining visual and textual elements to create a playful, humorous effect.\\n'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_texts[\"chatgpt_1b.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bcf3b2-e387-4c85-834c-e95066d420c0",
   "metadata": {},
   "source": [
    "## 9. Get WordNet synsets from ChatGPT texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "225b42d7-193e-4bc5-b8e9-7fccdd94aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from rdflib import Graph\n",
    "from nltk import word_tokenize, pos_tag, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e310f47b-c889-4f6c-84c2-f494b670455f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N5aa1d8caa31443a89c0cfeae9c2a3bf2 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse(\"../data/ontox_kg.ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "712c57b4-3b8a-4155-8dd2-2c0bbaeff6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [ \"visual\", \"textual\", \"combined\" ]\n",
    "\n",
    "def split_text(chatgpt_text):\n",
    "    chatgpt_texts_split = { mode: \"\" for mode in modes }\n",
    "    mode = 0\n",
    "    for line in chatgpt_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line == \"### 1. Interpretation of the Image\":\n",
    "            mode = 0\n",
    "        elif line == \"### 2. Interpretation of the Text\":\n",
    "            mode = 1\n",
    "        elif line == \"### 3. Interpretation of the Combination\":\n",
    "            mode = 2\n",
    "        else:\n",
    "            chatgpt_texts_split[modes[mode]] += \"\\n\" + line\n",
    "    return chatgpt_texts_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0f988ede-1492-4f31-9d92-13a2826ea64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meme_text_from_chatgpt_text(text):\n",
    "    meme_text = \"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if regex.search(\"\\\"\", line):\n",
    "            line = regex.sub('\" and \"', r\"\\n\", line)\n",
    "            line = regex.sub('^[^\"]*\"', \"\", line)\n",
    "            line = regex.sub(\"\\\".*$\", \"\", line)\n",
    "            meme_text = line\n",
    "            break\n",
    "    return meme_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "841f0023-1004-45db-a740-0ec7d3d3782b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_wikidata_info(entity_label):\n",
    "    \"\"\"\n",
    "    Function to query Wikidata API for a given entity label and return its QID and name.\n",
    "\n",
    "    Args:\n",
    "    - `entity_label` (str): Label of the entity to be queried.\n",
    "    \n",
    "    Returns:\n",
    "    - `str`: QID of the entity.\n",
    "    - `str`: Name of the entity.\n",
    "    \n",
    "    Dependencies:\n",
    "    - `requests`: For querying Wikidata API.\n",
    "    \n",
    "    Output:\n",
    "    - Returns the QID and name of the entity if found, otherwise returns `None`.\n",
    "    \n",
    "    \"\"\"\n",
    "    url = f\"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"en\",\n",
    "        \"search\": entity_label\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    if data['search']:\n",
    "        qid = data['search'][0]['id']\n",
    "        name = data['search'][0]['label']\n",
    "        return qid, name\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1461f954-56b4-4c47-bc1c-38603c411cef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_named_entities(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    unique_entities = set(entities)\n",
    "    entity_info = []\n",
    "    for entity, entity_type in unique_entities:\n",
    "        qid, name = get_wikidata_info(entity)\n",
    "        if qid and name:\n",
    "            entity_info.append((qid, name))\n",
    "    return sorted(entity_info, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7bba1fa2-0ea8-45f9-ad50-b1e849cf42b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_synsets(paragraph):\n",
    "    tokens = word_tokenize(paragraph)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    relevant_synsets = set()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token, tag in tagged_tokens:\n",
    "        if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ') or tag.startswith('RB'):\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            synsets = wn.synsets(lemma)\n",
    "            if synsets:\n",
    "                synset = synsets[0]\n",
    "                relevant_synsets.add((synset.name(), synset.definition()))\n",
    "    return sorted(list(relevant_synsets), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c5bbf767-f88b-4a41-9dcb-4d0f30672141",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontox_dict = json.load(open('../../data/ontox_dict.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "828e341e-a678-40d8-ad20-90b50d428832",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_ontox_dict = {}\n",
    "for file_name in chatgpt_texts:\n",
    "    chatgpt_text_split = split_text(chatgpt_texts[file_name])\n",
    "    meme_text = get_meme_text_from_chatgpt_text(chatgpt_text_split[\"textual\"])\n",
    "    dict_key = \"_\".join([chatgpt_dir, file_name])\n",
    "    linked_ontox_dict[dict_key] = {\n",
    "        \"Image_URL\": \"unknown\",\n",
    "        \"Meme_text\": meme_text,\n",
    "        \"Visual_description\": chatgpt_text_split[\"visual\"],\n",
    "        \"Textual_description\": chatgpt_text_split[\"textual\"],\n",
    "        \"Combined_description\": chatgpt_text_split[\"combined\"],\n",
    "        \"extracted_synsets\": {},\n",
    "        \"extracted_ne_qids\": {}\n",
    "    }\n",
    "    for mode in modes:\n",
    "        synsets = extract_synsets(chatgpt_text_split[mode])\n",
    "        named_entities = get_named_entities(chatgpt_text_split[mode])\n",
    "        linked_ontox_dict[dict_key][\"extracted_synsets\"][mode] = [{\"name\": synset[0], \"definition\": synset[1]} for synset in synsets]\n",
    "        linked_ontox_dict[dict_key][\"extracted_ne_qids\"][mode] = [{\"qid\": entity[0], \"name\": entity[1]} for entity in named_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "95c58fd3-f301-4428-8331-807baaa1246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_handle = open(\"gpahe_process.json\", \"w\")\n",
    "json.dump(linked_ontox_dict, outfile_handle)\n",
    "outfile_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68476aef-a07e-435c-a7e3-bfdeefd1f822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
