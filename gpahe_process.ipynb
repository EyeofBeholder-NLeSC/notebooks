{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7beda2b5-3c6b-43b7-9465-7bc753976b30",
   "metadata": {},
   "source": [
    "# Process GPAHE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0631237a-407e-424c-8173-66ab2922ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from IPython.display import Image\n",
    "from wikidata.client import Client\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c37271-6d39-47cf-bb8a-5febbfad283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"../../data/scraped_symbol_dict.json\"\n",
    "data_df = pd.read_json(DATA_FILE).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868c8491-144f-4a9a-8030-7e9632efa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: \n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9b582-5c8e-428f-9ec3-1c84a0aeebf3",
   "metadata": {},
   "source": [
    "## 1. Cluster hate symbols\n",
    "\n",
    "Use https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68bd9c4-8afb-4179-a798-0cc455175801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import regex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a9d7c-215b-4ae7-a49a-ceb3e776e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_data(data_df):\n",
    "    texts = []\n",
    "    for _, row in data_df.iterrows():\n",
    "        description = row[\"Description\"]\n",
    "        ideology = \". ideology is \" + regex.sub(\",\", \" . ideology is\", row[\"Ideology\"])\n",
    "        location = \". location is \" + regex.sub(\",\", \" . location is\", row[\"Location\"])\n",
    "        texts.append(\" \".join(word_tokenize(\" \".join([description, ideology, location]))))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c55a1-2b12-41b9-90f0-c3c5dfb5992f",
   "metadata": {},
   "source": [
    "Interesting words for clustering:\n",
    "* chapter\n",
    "* club/klub\n",
    "* group\n",
    "* organization\n",
    "* party\n",
    "* proud (boys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6067fc7-f7a9-4895-b3ad-d8a41fb28e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text_data(texts, nbr_of_dimensions=5):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    vectorized_data = TruncatedSVD(n_components=nbr_of_dimensions, n_iter=5, random_state=42)\n",
    "    vectorized_data.fit(X.T)\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9636a-cdc3-4245-9046-623df8a415c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(texts, vectorized_data, labels, dimension_1=0, dimension_2=1):\n",
    "    try:\n",
    "        x = vectorized_data.components_[dimension_1]\n",
    "        y = vectorized_data.components_[dimension_2]\n",
    "    except:\n",
    "        raise ValueError(f\"invalid pair of dimensions ({dimension_1}, {dimension_2})\")\n",
    "    split_data, labels = split_data_by_content(x, y, texts, labels)\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    for color in sorted(split_data.keys(), reverse=True):\n",
    "        plt.scatter(split_data[color][0], split_data[color][1], c=color, label=color, alpha=0.5)\n",
    "    for i in range(0, len(x)):\n",
    "        plt.annotate(str(i), (x[i], y[i]))\n",
    "    plt.legend(labels=[ label for color, label in sorted(labels.items(), reverse=True) ] )\n",
    "    plt.title(f\"{len(x)} hate symbols clustered by description, ideology and location\")\n",
    "    plt.savefig(\"gpahe_process.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf9523-88e5-4309-8fa8-0dd1cbff8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_token = \"proud boys\"\n",
    "orange_token = \"chapter\"\n",
    "yellow_token = \"club\"\n",
    "green_token = \"group\"\n",
    "other_color = \"blue\"\n",
    "labels = { \"yellow\": yellow_token, \"red\": red_token, \"orange\": orange_token, \"green\": green_token, other_color: \"other\" }\n",
    "\n",
    "def split_data_by_content(x, y, texts, labels):\n",
    "    split_data = { other_color: [[], []], \"green\": [[], []], \"orange\": [[], []], \"red\": [[], []], \"yellow\": [[], []] }\n",
    "    for i in range(0, len(x)):\n",
    "        if regex.search(red_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"red\"][0].append(x[i])\n",
    "            split_data[\"red\"][1].append(y[i])\n",
    "        elif regex.search(orange_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"orange\"][0].append(x[i])\n",
    "            split_data[\"orange\"][1].append(y[i])\n",
    "        elif regex.search(yellow_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"yellow\"][0].append(x[i])\n",
    "            split_data[\"yellow\"][1].append(y[i])\n",
    "        elif regex.search(green_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"green\"][0].append(x[i])\n",
    "            split_data[\"green\"][1].append(y[i])\n",
    "        else:\n",
    "            split_data[other_color][0].append(x[i])\n",
    "            split_data[other_color][1].append(y[i])\n",
    "    for color in labels:\n",
    "        labels[color] += f\" ({len(split_data[color][0])})\"\n",
    "    return split_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8a54c-e85d-429e-9957-3813e363a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_text_data(data_df)\n",
    "vectorized_data = vectorize_text_data(texts)\n",
    "visualize(texts, vectorized_data, labels.copy(), dimension_1=1, dimension_2=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4efd3d-b836-4500-9502-6be3bdb9d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.iloc[422][\"Description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975b3cd-85b7-4504-8b7c-b4b189a12fdc",
   "metadata": {},
   "source": [
    "## 2. Make a knowledge base\n",
    "\n",
    "* https://medium.com/nlplanet/building-a-knowledge-base-from-texts-a-full-practical-example-8dbbffb912fa\n",
    "* https://neo4j.com/blog/text-to-knowledge-graph-information-extraction-pipeline/\n",
    "* https://neo4j.com/developer-blog/construct-knowledge-graphs-unstructured-text/\n",
    "* https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a449e-e629-4ee8-b02c-c6b2d1a67cab",
   "metadata": {},
   "source": [
    "## 3. Extract entities with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fd6be-43fc-4f22-865f-ebeb26c42ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_FIELD_NAMES = \"CARDINAL DATE EVENT FAC GPE LANGUAGE LAW LOC MONEY NOMINAL NORP ORDINAL ORG PERCENT PERSON PRODUCT QUANTITY TIME WORK_OF_ART\".split()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba3d19-355f-436c-bd86-a1f929a68f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_values = { \"organizations\": [\"NORP\", \"ORG\" ],\n",
    "                 \"locations\": [ \"FAC\", \"GPE\", \"LOC\", ], \n",
    "                 \"events\": [ \"EVENT\", ],\n",
    "                 \"dates\": [ \"DATE\", \"TIME\", ], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6dbe2a-0378-47c1-adc8-9f0a41698aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(nlp_analysis):\n",
    "    entities = {}\n",
    "    for entity in nlp_analysis.ents:\n",
    "        if entity.label_ not in entities:\n",
    "            entities[entity.label_] = []\n",
    "        entities[entity.label_].append(entity.text)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15c641-f955-4118-ad8d-8dd5c350ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_analysis(texts, nlp):\n",
    "    entity_data = []\n",
    "    for text in texts:\n",
    "        nlp_analysis = nlp(text) \n",
    "        entities = get_entities(nlp_analysis)\n",
    "        entity_data.append(entities)\n",
    "    return entity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b2e82-d2ca-4077-a248-af43ac9a3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entity_groups(entity_data, entity_group_name, n=10):\n",
    "    values = {}\n",
    "    for entities in entity_data:\n",
    "        for entity_name in entities:\n",
    "            if entity_name in field_values[entity_group_name]:\n",
    "                for entity in entities[entity_name]:\n",
    "                    if entity in values:\n",
    "                        values[entity] += 1\n",
    "                    else:\n",
    "                        values[entity] = 1\n",
    "    print(f\"{entity_group_name} ({sum(values.values())}):\", [[key, value] for key, value in sorted(values.items(), key=lambda item: item[1], reverse=True)][:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5b9b4-c52e-4c2b-9463-9f83bf5575cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entities(entity_data, target_entity_name, n=10):\n",
    "    values = {}\n",
    "    for entities in entity_data:\n",
    "        for entity_name in entities:\n",
    "            if entity_name == target_entity_name:\n",
    "                for entity in entities[entity_name]:\n",
    "                    if entity in values:\n",
    "                        values[entity] += 1\n",
    "                    else:\n",
    "                        values[entity] = 1\n",
    "    print(f\"{target_entity_name} ({sum(values.values())}):\", [[key, value] for key, value in sorted(values.items(), key=lambda item: item[1], reverse=True)][:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bf4d1-1c22-456c-b63e-fbc041a8f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data = nlp_analysis(texts, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dddc96-bf7c-4016-b316-7eed2ee8d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field_name in SPACY_FIELD_NAMES:\n",
    "    count_entities(entity_data, field_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35f45c-f39e-4a49-adde-43ccfcf469a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"organizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcd6fc-8f02-4571-90e7-1301ab080712",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de616c-e236-4aca-879d-c63636ea780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e35ad4-dc31-4cd3-8a6d-56e9882b55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31707462-0f3d-40e1-b159-74dc18e11611",
   "metadata": {},
   "source": [
    "## 4. Extract entities with REBEL\n",
    "\n",
    "Based on blog by Fabio Chiusano: https://medium.com/nlplanet/building-a-knowledge-base-from-texts-a-full-practical-example-8dbbffb912fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90fa06-3867-47a9-98b3-28ef5c9bdf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "import wikipedia\n",
    "from newspaper import Article, ArticleException\n",
    "from GoogleNews import GoogleNews\n",
    "import IPython\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe418b8-b76c-4517-a636-4becbefa0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976491bc-7807-489c-ba1c-21940fd8f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        relations.append({\n",
    "            'head': subject.strip(),\n",
    "            'type': relation.strip(),\n",
    "            'tail': object_.strip()\n",
    "        })\n",
    "    return relations\n",
    "\n",
    "# source: https://gist.githubusercontent.com/fabiochiusano/934ad5ff318626befbdd20c72e074186/raw/e3e44110a0db5408d17fba52be559ecaf676b6d2/kb_4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa85990-f73e-44bc-a7d4-31d1d83e07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB():\n",
    "    def __init__(self):\n",
    "        self.relations = []\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "    def add_relation(self, r):\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")\n",
    "\n",
    "# source: https://gist.githubusercontent.com/fabiochiusano/e64d5250371e18f7a6cc02ac0cdc64c5/raw/24af0f7f23b313591fe91fc9f8826cf216ca4568/kb_5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4edd6d-e03c-494f-992f-b5b5288f5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_small_text_to_kb(text, verbose=False):\n",
    "    kb = KB()\n",
    "\n",
    "    # Tokenizer text\n",
    "    model_inputs = tokenizer(text, max_length=512, padding=True, truncation=True,\n",
    "                            return_tensors='pt')\n",
    "    if verbose:\n",
    "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
    "\n",
    "    # Generate\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 216,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": 3\n",
    "    }\n",
    "    generated_tokens = model.generate(\n",
    "        **model_inputs,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "    # create kb\n",
    "    for sentence_pred in decoded_preds:\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for r in relations:\n",
    "            kb.add_relation(r)\n",
    "\n",
    "    return kb\n",
    "\n",
    "# source: https://gist.githubusercontent.com/fabiochiusano/ceec4d9ff1ce2ad25c40fbd8412aa9e4/raw/796771f88776fca9d7c4c84bd1b3a52d9ef5b5c1/kb_6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de33da-7d9c-41e1-a0dc-cdc60890cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_per_text(texts):\n",
    "    relations_per_text = []\n",
    "    for text in texts:\n",
    "        relations = []\n",
    "        for sentence in sent_tokenize(text):\n",
    "            kb = from_small_text_to_kb(sentence, verbose=True)\n",
    "            relations.extend(kb.__dict__[\"relations\"])\n",
    "        relations_per_text.append(relations)\n",
    "        squeal(f\"{len(relations_per_text)}: {sum([len(relations) for relations in relations_per_text])/len(relations_per_text)}\")\n",
    "    return relations_per_text\n",
    "\n",
    "# source for line 6: https://gist.githubusercontent.com/fabiochiusano/a720da218ee8d19de3130fa36c23a69b/raw/a9b94a3ddbad61cfb3713234476423fffbfdca41/kb_7.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61952a3-931c-44ee-ad64-d67f7eb4e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_relations(relations_per_text):\n",
    "    relations_count = {}\n",
    "    for relations in relations_per_text:\n",
    "        for relation in relations:\n",
    "            key = \"#\".join([relation[\"head\"], relation[\"type\"], relation[\"tail\"]])\n",
    "            if key in relations_count:\n",
    "                relations_count[key] += 1\n",
    "            else:\n",
    "                relations_count[key] = 1\n",
    "    return relations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc33865-41da-45fb-9400-28384d861fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relations_to_df(relations_per_text):\n",
    "    relations_list = []\n",
    "    for counter in range(0, len(relations_per_text)):\n",
    "        for relation in relations_per_text[counter]:\n",
    "            relation[\"text_id\"] = counter\n",
    "            relations_list.append(relation)\n",
    "    relations_df = pd.DataFrame(relations_list)\n",
    "    return relations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8237c-c974-444f-84c3-35c5fe14a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_of_lists(dict_of_lists):\n",
    "    return [ (len(list), key) for key, list in sorted(dict_of_lists.items(), key=lambda x: len(x[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd6932-9815-4421-88a9-ea327df2ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicates(relations_df):\n",
    "    nbr_of_duplicates = 0\n",
    "    seen = {}\n",
    "    for index, relation in relations_df.iterrows():\n",
    "        key = \"#\".join([relation[\"head\"], relation[\"type\"], relation[\"type\"]])\n",
    "        if key in seen:\n",
    "            nbr_of_duplicates += 1\n",
    "        seen[key] = True\n",
    "    return nbr_of_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a8205-1dc3-46b1-afb2-cc8c243a6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_relation_fields(relations_df):\n",
    "    head_terms = {}\n",
    "    tail_terms = {}\n",
    "    type_head_terms = {}\n",
    "    type_tail_terms = {}\n",
    "\n",
    "    for index, relation in relations_df.iterrows():\n",
    "        head = relation[\"head\"]\n",
    "        tail = relation[\"tail\"]\n",
    "        type = relation[\"type\"]\n",
    "        if head not in head_terms:\n",
    "            head_terms[head] = []\n",
    "        if tail not in head_terms[head]:\n",
    "            head_terms[head].append(tail)\n",
    "        if tail not in tail_terms:\n",
    "            tail_terms[tail] = []\n",
    "        if head not in tail_terms[tail]:\n",
    "            tail_terms[tail].append(head)\n",
    "        key = f\"{type}({head},_)\"\n",
    "        if key not in type_head_terms:\n",
    "            type_head_terms[key] = []\n",
    "        if tail not in type_head_terms[key]:\n",
    "            type_head_terms[key].append(tail)\n",
    "        key = f\"{type}(_,{tail})\"\n",
    "        if key not in type_tail_terms:\n",
    "            type_tail_terms[key] = []\n",
    "        if head not in type_tail_terms[key]:\n",
    "            type_tail_terms[key].append(head)\n",
    "    return head_terms, tail_terms, type_head_terms, type_tail_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98fbe3-b1d9-48d4-878b-190d2106103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_per_text = extract_relations_per_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0751015-edda-4152-8b67-ffbe03b5e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_count = count_relations(relations_per_text)\n",
    "print(f\"number of relations: {sum(relations_count.values())}; unique: {len(relations_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5f101-2f5f-4fd2-8110-9780c77a059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_freqs(relations_count)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe522989-9689-4af2-9d5a-805c3efb8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df = convert_relations_to_df(relations_per_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffcda8-8bab-459f-bfa8-c1c5307c1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df.to_csv(\"gpahe_process.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee6172-a257-4967-84a5-2dfa8d0a6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_terms, tail_terms, type_head_terms, type_tail_terms = count_relation_fields(relations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583abde0-22c5-454e-8bf9-cc56a5cee7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(head_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef68304-e25f-471a-bd36-5e35b05720f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(tail_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0abac8-f7d6-4ea7-a1fe-1751b0d6c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(type_head_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f5d5f-3fe5-4d68-ab2b-d6eb86f192eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(type_tail_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5b912-fa54-42a1-a5ea-5c6eb2dbd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f210e0-f9b4-486c-804b-4230c4172e27",
   "metadata": {},
   "source": [
    "## 5. Visualize knowlegde triples\n",
    "\n",
    "To do: add labels to edges (note: an edge can have several labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda225b-09cc-4dec-a436-17f6f74b4b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec5459-7768-45dd-af33-a8509804465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(relations_df, source=\"head\", target=\"tail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d803385-77d0-4d91-9424-7564bcbe812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(notebook=True)\n",
    "net.from_nx(G)\n",
    "net.show(\"gpahe_process.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606e82b-0198-48d0-a33d-5c90f2c33a24",
   "metadata": {},
   "source": [
    "## 6. Link to wikidata (fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787b9b8-8346-4e20-9483-9b028bba6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()  # doctest: +SKIP\n",
    "entity = client.get('Q20145', load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eec733-ee86-4966-b77c-fbd2fe1c9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f79870-0780-41f6-9ba1-6f35dc4214ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "from pywikibot import pagegenerators, WikidataBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b10966-6ae2-410e-9755-a55dbc6e742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = \"SELECT ?item WHERE { ?item rdfs:label 'Google'@en }\"\n",
    "entities = pagegenerators.WikidataSPARQLPageGenerator(sparql)\n",
    "entities = list(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa72825-6b52-486e-8879-e269ad83e7ff",
   "metadata": {},
   "source": [
    "## 7. download logo's manually (lot of work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773f3e21-55f6-44e3-adeb-638b669f0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2ffa22-a21a-45e7-addb-751c8171a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://symbols.globalextremism.org/details?recordId=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0b558a6-8dd1-4e6d-a2b3-4f641c712899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9560/1927802637.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m        \u001b[0mwebbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m        \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/eye/venv3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStdinNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1262\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/eye/venv3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "COUNTER_MINIMUM = 80\n",
    "counter = 0\n",
    "for id in data_df.index:\n",
    "    counter += 1\n",
    "    if counter > COUNTER_MINIMUM:\n",
    "       webbrowser.open(base_url + id, new=2)\n",
    "       print(counter)\n",
    "       input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3467e9-0196-46ce-84bc-2d54e2953ee4",
   "metadata": {},
   "source": [
    "1. open the web page in a new tab of a browser\n",
    "2. right click on the logo/image\n",
    "3. open the image in a new tab of the browser\n",
    "4. right click on the image\n",
    "5. save the image with name number.extension\n",
    "6. close the two added tabs\n",
    "7. push the enter/return button on the notebook page\n",
    "8. repeat for the next logo/image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5e1e48d-94ac-473c-a067-a1ff49a0d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = os.environ[\"HOME\"] + \"/Downloads\"\n",
    "download_files = os.listdir(download_dir)\n",
    "extensions = [ \"jpg\", \"png\", \"svg\", \"webp\", \"JPG\", \"PNG\", \"SVG\", \"WEBP\" ]  \n",
    "counter = 0\n",
    "for id in data_df.index:\n",
    "    counter += 1\n",
    "    if counter > COUNTER_MINIMUM:\n",
    "        break\n",
    "    file_found = False\n",
    "    for extension in extensions:\n",
    "        if os.path.isfile(download_dir + \"/\" + str(counter) + \".\" + extension):\n",
    "            file_found = True\n",
    "            break\n",
    "    if not file_found:\n",
    "        print(f\"cannot find file number {counter}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d243e-23ae-4e53-935a-69502938068f",
   "metadata": {},
   "source": [
    "## 8. Link Spacy entities from ChatGPT output to GPAHE metedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c85de4-b840-4cbd-b6be-c15e71af29c0",
   "metadata": {},
   "source": [
    "From the Spacy analysis we select all:\n",
    "1. noun phrases\n",
    "2. entities\n",
    "3. tokens with pos tag PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "76c13708-2209-4376-8019-f480b2ad18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_chars = str.maketrans(\"*#\", \"  \")\n",
    "\n",
    "def get_phrases(text, spacy_model):\n",
    "    nlp_analysis = spacy_model(text.translate(remove_chars))\n",
    "    chunk_texts = [ regex.sub(\"^[Tt][Hh][EeIi][Ss]* \", \"\", \n",
    "                        regex.sub(\"^[Aa][Nn]* \", \"\", chunk.text, \n",
    "                                  regex.IGNORECASE), \n",
    "                              regex.IGNORECASE) \n",
    "                     for chunk in nlp_analysis.noun_chunks\n",
    "                  ]\n",
    "    chunk_texts.extend([entity.text for entity in nlp_analysis.ents])\n",
    "    chunk_texts.extend([token.text for token in nlp_analysis if token.pos_ == \"PROPN\" ])\n",
    "    return chunk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3a4aced-22ea-423c-b5c6-c9e86998f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_list(data_df):\n",
    "    term_dict = {}\n",
    "    for index, row in data_df.iterrows():\n",
    "        term_dict[row[\"Title\"]] = True\n",
    "        for ideology in row[\"Ideology\"].split(\",\"):\n",
    "            term_dict[ideology.strip()] = True\n",
    "        for location in row[\"Location\"].split(\",\"):\n",
    "            term_dict[location.strip()] = True\n",
    "    return sorted(term_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "dee8f1d6-58b1-4b3c-884d-798561d90133",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_dir = \"chatgpt\"\n",
    "\n",
    "def read_chatgpt_texts(chatgpt_dir):\n",
    "    chatgpt_files = [ file_name for file_name in os.listdir(chatgpt_dir)\n",
    "                      if regex.search(\"b.txt\", file_name) ]\n",
    "    chatgpt_texts = {}\n",
    "    for file_name in sorted(chatgpt_files):\n",
    "        file_handle = open(os.path.join(chatgpt_dir, file_name), \"r\")\n",
    "        lines = file_handle.readlines()\n",
    "        file_handle.close()\n",
    "        chatgpt_texts[\"_\".join([chatgpt_dir, file_name])] = \" \".join(lines)\n",
    "    return chatgpt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f0257e06-3b44-4cac-a127-3635c792caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "term_list = get_term_list(data_df)\n",
    "chatgpt_texts = read_chatgpt_texts(chatgpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "51d0e06e-4f79-4aa8-87a4-37c101ab0a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4/20']\n",
      "['neo-Nazi']\n",
      "['Pepe the Frog']\n",
      "['Germany', 'Nazi']\n"
     ]
    }
   ],
   "source": [
    "term_list_lower = [ term.lower() for term in term_list ]\n",
    "meme_texts = {}\n",
    "for file_name in sorted(chatgpt_texts.keys()):\n",
    "    phrases = list(set(get_phrases(chatgpt_texts[file_name], spacy_model)))\n",
    "    phrases_in_term_list = [ phrase for phrase in phrases if phrase.lower() in term_list_lower ]\n",
    "    meme_texts[\"_\".join([chatgpt_dir, file_name])] = get_meme_text_from_chatgpt_text(chatgpt_texts[file_name])\n",
    "    print(phrases_in_term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9251296f-8374-4db0-9096-5af061db1af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 1. Interpretation of the Image\\n The image shows a dog with a happy, slightly mischievous expression. The background is decorated with colorful, psychedelic patterns that resemble marijuana leaves, often associated with a state of altered consciousness or celebration.\\n \\n ### 2. Interpretation of the Text\\n The text reads: \"ITS GONNA BE 4/20 FOR A WHOLE MONTH.\" This is a play on the date April 20th (4/20), which is widely recognized in cannabis culture as a day for celebrating and consuming marijuana.\\n \\n ### 3. Interpretation of the Combination\\n The combination of the happy, relaxed dog and the text implies a humorous and exaggerated scenario where the state of celebration and relaxation associated with 4/20 lasts for an entire month. The dog\\'s expression, along with the colorful background, reinforces the playful and light-hearted tone of the message, suggesting an extended period of enjoyment and leisure.\\n \\n In essence, the meme is using humor to exaggerate the idea of an extended celebration of cannabis culture, combining visual and textual elements to create a playful, humorous effect.\\n'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_texts[\"chatgpt_1b.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bcf3b2-e387-4c85-834c-e95066d420c0",
   "metadata": {},
   "source": [
    "## 9. Get WordNet synsets from ChatGPT texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "225b42d7-193e-4bc5-b8e9-7fccdd94aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from rdflib import Graph\n",
    "from nltk import word_tokenize, pos_tag, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e310f47b-c889-4f6c-84c2-f494b670455f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N5aa1d8caa31443a89c0cfeae9c2a3bf2 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse(\"../data/ontox_kg.ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "712c57b4-3b8a-4155-8dd2-2c0bbaeff6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [ \"visual\", \"textual\", \"combined\" ]\n",
    "\n",
    "def split_text(chatgpt_text):\n",
    "    chatgpt_texts_split = { mode: \"\" for mode in modes }\n",
    "    mode = 0\n",
    "    for line in chatgpt_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line == \"### 1. Interpretation of the Image\":\n",
    "            mode = 0\n",
    "        elif line == \"### 2. Interpretation of the Text\":\n",
    "            mode = 1\n",
    "        elif line == \"### 3. Interpretation of the Combination\":\n",
    "            mode = 2\n",
    "        else:\n",
    "            chatgpt_texts_split[modes[mode]] += \"\\n\" + line\n",
    "    return chatgpt_texts_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0f988ede-1492-4f31-9d92-13a2826ea64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meme_text_from_chatgpt_text(text):\n",
    "    meme_text = \"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if regex.search(\"\\\"\", line):\n",
    "            line = regex.sub('\" and \"', r\"\\n\", line)\n",
    "            line = regex.sub('^[^\"]*\"', \"\", line)\n",
    "            line = regex.sub(\"\\\".*$\", \"\", line)\n",
    "            meme_text = line\n",
    "            break\n",
    "    return meme_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "841f0023-1004-45db-a740-0ec7d3d3782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_info(entity_label):\n",
    "    \"\"\"\n",
    "    Function to query Wikidata API for a given entity label and return its QID and name.\n",
    "\n",
    "    Args:\n",
    "    - `entity_label` (str): Label of the entity to be queried.\n",
    "    \n",
    "    Returns:\n",
    "    - `str`: QID of the entity.\n",
    "    - `str`: Name of the entity.\n",
    "    \n",
    "    Dependencies:\n",
    "    - `requests`: For querying Wikidata API.\n",
    "    \n",
    "    Output:\n",
    "    - Returns the QID and name of the entity if found, otherwise returns `None`.\n",
    "    \n",
    "    \"\"\"\n",
    "    url = f\"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"en\",\n",
    "        \"limit\": 100,\n",
    "        \"uselang\": \"en\",\n",
    "        \"search\": entity_label\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    if 'search' in data.keys():\n",
    "        return data[\"search\"]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "b65441f5-7798-46e7-a860-bdc21925f02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Q15994152',\n",
       "  'title': 'Q15994152',\n",
       "  'pageid': 17611956,\n",
       "  'concepturi': 'http://www.wikidata.org/entity/Q15994152',\n",
       "  'repository': 'wikidata',\n",
       "  'url': '//www.wikidata.org/wiki/Q15994152',\n",
       "  'display': {'label': {'value': 'Racial Holy War', 'language': 'en'},\n",
       "   'description': {'value': 'White Supremacist concept', 'language': 'en'}},\n",
       "  'label': 'Racial Holy War',\n",
       "  'description': 'White Supremacist concept',\n",
       "  'match': {'type': 'label', 'language': 'en', 'text': 'Racial Holy War'}},\n",
       " {'id': 'Q77977913',\n",
       "  'title': 'Q77977913',\n",
       "  'pageid': 77436094,\n",
       "  'concepturi': 'http://www.wikidata.org/entity/Q77977913',\n",
       "  'repository': 'wikidata',\n",
       "  'url': '//www.wikidata.org/wiki/Q77977913',\n",
       "  'display': {'label': {'value': 'Racial Holy War: The Cold War',\n",
       "    'language': 'en'},\n",
       "   'description': {'value': '2016 first-person shooter video game',\n",
       "    'language': 'en'}},\n",
       "  'label': 'Racial Holy War: The Cold War',\n",
       "  'description': '2016 first-person shooter video game',\n",
       "  'match': {'type': 'label',\n",
       "   'language': 'en',\n",
       "   'text': 'Racial Holy War: The Cold War'}},\n",
       " {'id': 'Q1819273',\n",
       "  'title': 'Q1819273',\n",
       "  'pageid': 1750135,\n",
       "  'concepturi': 'http://www.wikidata.org/entity/Q1819273',\n",
       "  'repository': 'wikidata',\n",
       "  'url': '//www.wikidata.org/wiki/Q1819273',\n",
       "  'display': {'label': {'value': 'Ben Klassen', 'language': 'en'},\n",
       "   'description': {'value': 'American engineer, author and politician (1918-1993)',\n",
       "    'language': 'en'}},\n",
       "  'label': 'Ben Klassen',\n",
       "  'description': 'American engineer, author and politician (1918-1993)',\n",
       "  'match': {'type': 'label', 'language': 'sv', 'text': 'Racial Holy War'},\n",
       "  'aliases': ['Racial Holy War']},\n",
       " {'id': 'Q679584',\n",
       "  'title': 'Q679584',\n",
       "  'pageid': 640206,\n",
       "  'concepturi': 'http://www.wikidata.org/entity/Q679584',\n",
       "  'repository': 'wikidata',\n",
       "  'url': '//www.wikidata.org/wiki/Q679584',\n",
       "  'display': {'label': {'value': 'Creativity', 'language': 'en'},\n",
       "   'description': {'value': 'pantheistic white separatist religious movement, founded in Lighthouse Point, Florida by Ben Klassen in 1973; promotes the veneration of the white race and the safeguarding of its survival',\n",
       "    'language': 'en'}},\n",
       "  'label': 'Creativity',\n",
       "  'description': 'pantheistic white separatist religious movement, founded in Lighthouse Point, Florida by Ben Klassen in 1973; promotes the veneration of the white race and the safeguarding of its survival',\n",
       "  'match': {'type': 'alias', 'language': 'fi', 'text': 'Racial holy war'},\n",
       "  'aliases': ['Racial holy war']}]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wikidata_info(\"Racial Holy War\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "1461f954-56b4-4c47-bc1c-38603c411cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entities(paragraph, spacy_model):\n",
    "    doc = spacy_model(paragraph)\n",
    "    entity_info = []\n",
    "    for entity_text in set([entity.text for entity in doc.ents]):\n",
    "        wikidata_info = get_wikidata_info(entity_text)\n",
    "        for wikidata_item in wikidata_info:\n",
    "            entity_info.append((wikidata_item[\"id\"], wikidata_item[\"label\"]))\n",
    "            if \"aliases\" in wikidata_item:\n",
    "                print(wikidata_item[\"label\"], wikidata_item[\"aliases\"])\n",
    "                for label in wikidata_item[\"aliases\"]:\n",
    "                    entity_info.append((wikidata_item[\"id\"], label))\n",
    "    return sorted(set(entity_info), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7bba1fa2-0ea8-45f9-ad50-b1e849cf42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_synsets(paragraph):\n",
    "    tokens = word_tokenize(paragraph)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    relevant_synsets = set()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token, tag in tagged_tokens:\n",
    "        if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ') or tag.startswith('RB'):\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            synsets = wn.synsets(lemma)\n",
    "            if synsets:\n",
    "                synset = synsets[0]\n",
    "                relevant_synsets.add((synset.name(), synset.definition()))\n",
    "    return sorted(list(relevant_synsets), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c5bbf767-f88b-4a41-9dcb-4d0f30672141",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontox_dict = json.load(open('../../data/ontox_dict.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "828e341e-a678-40d8-ad20-90b50d428832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannabis sativa ['marijuana']\n",
      "cannabis ['marijuana']\n",
      "Cannabis ['marijuana']\n",
      "420 ['4/20']\n",
      "Cannabis sativa ['marijuana']\n",
      "cannabis ['marijuana']\n",
      "Cannabis ['marijuana']\n",
      "420 ['4/20']\n",
      "Castle Grayskull ['Grayskull']\n",
      "The Taking of Grayskull ['Grayskull wird verschleppt']\n",
      "The Power of Grayskull ['Grayskulls styrka']\n",
      "Power Sword ['Grayskullsvärdet']\n",
      "Ben Klassen ['Racial Holy War']\n",
      "Creativity ['Racial holy war']\n",
      "National Socialist black metal ['neo-Nazi black metal']\n",
      "white power skinhead ['neo-Nazi skinhead']\n",
      "Francesco Pepe ['Pepe']\n",
      "Francesco Pepe ['Pepe']\n",
      "Feels Good Man ['Pepe the Frog: Feels Good Man']\n",
      "Adolf Hitler ['Hitler']\n",
      "Nazi Germany [\"Hitler's Third Reich\"]\n",
      "Karim Mohamed ['Hitler']\n",
      "Theatre of War ['World War II RTS: Theatre of War']\n",
      "Theatre of War ['World War II']\n",
      "Nazi Era ['Nazi Germany']\n",
      "flag of Nazi Germany ['Nazi Germany flag']\n",
      "The Years of Extermination: Nazi Germany and the Jews, 1939-1945 ['Nazi Germany and the Jews: The Years of Exterminatio']\n",
      "science in Nazi Germany ['Nazi Germany and science']\n",
      "Kriegsmarine ['Nazi Navy']\n",
      "Adolf Hitler ['Hitler']\n",
      "Nazi Germany [\"Hitler's Third Reich\"]\n",
      "Karim Mohamed ['Hitler']\n",
      "April 20 ['April 20th']\n",
      "420 ['April 20th']\n",
      "Cannabis sativa ['marijuana']\n",
      "cannabis ['marijuana']\n",
      "Cannabis ['marijuana']\n",
      "Abgenix 4.209.3 ['4.209.3']\n",
      "04:20 ['4.20']\n",
      "Internet meme ['meme']\n",
      "Mohanad Ali ['meme']\n",
      "Emmanuel del Real ['Meme del Real']\n",
      "Yahoo! Meme ['Meme']\n",
      "Abgenix 4.209.3 ['4.209.3']\n",
      "04:20 ['4.20']\n"
     ]
    }
   ],
   "source": [
    "linked_ontox_dict = {}\n",
    "for file_name in chatgpt_texts:\n",
    "    chatgpt_text_split = split_text(chatgpt_texts[file_name])\n",
    "    meme_text = get_meme_text_from_chatgpt_text(chatgpt_text_split[\"textual\"])\n",
    "    linked_ontox_dict[file_name] = {\n",
    "        \"Image_URL\": \"unknown\",\n",
    "        \"Meme_text\": meme_text,\n",
    "        \"Visual_description\": chatgpt_text_split[\"visual\"],\n",
    "        \"Textual_description\": chatgpt_text_split[\"textual\"],\n",
    "        \"Combined_description\": chatgpt_text_split[\"combined\"],\n",
    "        \"extracted_synsets\": {},\n",
    "        \"extracted_ne_qids\": {}\n",
    "    }\n",
    "    for mode in modes:\n",
    "        synsets = extract_synsets(chatgpt_text_split[mode])\n",
    "        named_entities = get_named_entities(chatgpt_text_split[mode], spacy_model)\n",
    "        linked_ontox_dict[file_name][\"extracted_synsets\"][mode] = [{\"name\": synset[0], \"definition\": synset[1]} for synset in synsets]\n",
    "        linked_ontox_dict[file_name][\"extracted_ne_qids\"][mode] = [{\"qid\": entity[0], \"name\": entity[1]} for entity in named_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "95c58fd3-f301-4428-8331-807baaa1246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_handle = open(\"gpahe_process.json\", \"w\")\n",
    "json.dump(linked_ontox_dict, outfile_handle)\n",
    "outfile_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "68476aef-a07e-435c-a7e3-bfdeefd1f822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Image_URL': 'unknown',\n",
       " 'Meme_text': 'I NEED RaHoWa!',\n",
       " 'Visual_description': '\\nThe image shows a character from the popular 1980s animated television series \"He-Man and the Masters of the Universe.\" He-Man is depicted raising his sword in front of Castle Grayskull, which is a central and iconic location in the series. The image often signifies He-Man calling upon the power of Grayskull, a moment of empowerment and transformation.\\n',\n",
       " 'Textual_description': '\\nThe text reads: \"I NEED RaHoWa!\" The term \"RaHoWa\" is highly controversial and stands for \"Racial Holy War.\" It is associated with white supremacist and neo-Nazi groups, used to promote their ideology of racial conflict. The term is problematic and offensive, advocating for extremist and violent beliefs.\\n',\n",
       " 'Combined_description': '\\nThe combination of He-Man, a character symbolizing strength and heroism, with the extremist term \"RaHoWa\" is jarring and inappropriate. This juxtaposition is likely intended to shock or provoke. Using a beloved pop culture figure in conjunction with a term that promotes hate and violence is a way to draw attention and create a stark contrast.\\n\\n### Conclusion\\nThe image misappropriates a well-known character for a harmful message. It\\'s important to recognize and condemn the use of such extremist language, even when presented in a seemingly innocuous or humorous context. This type of content can be harmful and spread negative ideologies under the guise of humor or nostalgia.\\n',\n",
       " 'extracted_synsets': {'visual': [{'name': 'authorization.n.04',\n",
       "    'definition': 'the act of conferring legality or sanction or formal warrant'},\n",
       "   {'name': 'be.v.01',\n",
       "    'definition': 'have the quality of being; (copula, used with an adjective or a predicate noun)'},\n",
       "   {'name': 'career.n.01',\n",
       "    'definition': 'the particular occupation for which you are trained'},\n",
       "   {'name': 'central.n.01',\n",
       "    'definition': 'a workplace that serves as a telecommunications facility where lines from telephones can be connected together to permit communication'},\n",
       "   {'name': 'elevation.n.01',\n",
       "    'definition': 'the event of something being raised upward'},\n",
       "   {'name': 'fictional_character.n.01',\n",
       "    'definition': 'an imaginary person represented in a work of fiction (play or film or story)'},\n",
       "   {'name': 'frequently.r.01', 'definition': 'many times at short intervals'},\n",
       "   {'name': 'front.n.01',\n",
       "    'definition': 'the side that is forward or prominent'},\n",
       "   {'name': 'iconic.a.01',\n",
       "    'definition': 'relating to or having the characteristics on an icon'},\n",
       "   {'name': 'image.n.01', 'definition': 'an iconic mental representation'},\n",
       "   {'name': 'inspire.v.01', 'definition': 'heighten or intensify'},\n",
       "   {'name': 'location.n.01', 'definition': 'a point or extent in space'},\n",
       "   {'name': 'masters.n.01', 'definition': 'United States poet (1869-1950)'},\n",
       "   {'name': 'mean.v.03', 'definition': 'denote or connote'},\n",
       "   {'name': 'moment.n.01', 'definition': 'a particular point in time'},\n",
       "   {'name': 'palace.n.01', 'definition': 'a large and stately mansion'},\n",
       "   {'name': 'picture.v.02', 'definition': 'show in, or as in, a picture'},\n",
       "   {'name': 'popular.a.01',\n",
       "    'definition': 'regarded with great favor, approval, or affection especially by the general public'},\n",
       "   {'name': 'power.n.01', 'definition': 'possession of controlling influence'},\n",
       "   {'name': 'series.n.01',\n",
       "    'definition': 'similar things placed in order or happening one after another'},\n",
       "   {'name': 'show.n.01',\n",
       "    'definition': 'the act of publicly exhibiting or entertaining'},\n",
       "   {'name': 'stud.n.01',\n",
       "    'definition': 'a man who is virile and sexually active'},\n",
       "   {'name': 'sword.n.01',\n",
       "    'definition': 'a cutting or thrusting weapon that has a long metal blade and a hilt with a hand guard'},\n",
       "   {'name': 'television.n.01',\n",
       "    'definition': 'broadcasting visual images of stationary or moving objects; ;  - Ernie Kovacs'},\n",
       "   {'name': 'transformation.n.01', 'definition': 'a qualitative change'},\n",
       "   {'name': 'universe.n.01', 'definition': 'everything that exists anywhere'}],\n",
       "  'textual': [{'name': 'associate.v.01',\n",
       "    'definition': 'make a logical or causal connection'},\n",
       "   {'name': 'base.n.08', 'definition': 'a support or foundation'},\n",
       "   {'name': 'be.v.01',\n",
       "    'definition': 'have the quality of being; (copula, used with an adjective or a predicate noun)'},\n",
       "   {'name': 'belief.n.01', 'definition': 'any cognitive content held as true'},\n",
       "   {'name': 'conflict.n.01',\n",
       "    'definition': 'an open clash between two opposing groups (or individuals); --Thomas Paine'},\n",
       "   {'name': 'controversial.a.01',\n",
       "    'definition': 'marked by or capable of arousing controversy'},\n",
       "   {'name': 'debatable.s.01', 'definition': 'open to doubt or debate'},\n",
       "   {'name': 'extremist.n.01',\n",
       "    'definition': 'a person who holds extreme views'},\n",
       "   {'name': 'group.n.01',\n",
       "    'definition': 'any number of entities (members) considered as a unit'},\n",
       "   {'name': 'highly.r.01',\n",
       "    'definition': 'to a high degree or extent; favorably or with much respect'},\n",
       "   {'name': 'holy_place.n.01', 'definition': 'a sacred place of pilgrimage'},\n",
       "   {'name': 'need.n.01', 'definition': 'a condition requiring relief'},\n",
       "   {'name': 'offense.n.05', 'definition': 'the action of attacking an enemy'},\n",
       "   {'name': 'political_orientation.n.01',\n",
       "    'definition': 'an orientation that characterizes the thinking of a group or nation'},\n",
       "   {'name': 'promote.v.01',\n",
       "    'definition': 'contribute to the progress or growth of'},\n",
       "   {'name': 'racial.a.01',\n",
       "    'definition': 'of or related to genetically distinguished groups of people'},\n",
       "   {'name': 'read.n.01', 'definition': 'something that is read'},\n",
       "   {'name': 'recommend.v.01', 'definition': 'push for something'},\n",
       "   {'name': 'supremacist.n.01',\n",
       "    'definition': 'a person who advocates the supremacy of some particular group or race over all others'},\n",
       "   {'name': 'term.n.01',\n",
       "    'definition': 'a word or expression used for some particular thing'},\n",
       "   {'name': 'text.n.01', 'definition': 'the words of something written'},\n",
       "   {'name': 'use.v.01',\n",
       "    'definition': 'put into service; make work or employ for a particular purpose or for its inherent or natural purpose'},\n",
       "   {'name': 'violent.a.01',\n",
       "    'definition': 'acting with or marked by or resulting from great force or energy or emotional intensity'},\n",
       "   {'name': 'war.n.01',\n",
       "    'definition': 'the waging of armed conflict against an enemy'},\n",
       "   {'name': 'white.n.01', 'definition': 'a member of the Caucasoid race'}],\n",
       "  'combined': [{'name': 'acknowledge.v.06',\n",
       "    'definition': 'accept (someone) to be what is claimed or accept his power and authority'},\n",
       "   {'name': 'apparently.r.01',\n",
       "    'definition': 'from appearances alone; ; ; -Thomas Hardy'},\n",
       "   {'name': 'arouse.v.01',\n",
       "    'definition': 'call forth (emotions, feelings, and responses)'},\n",
       "   {'name': 'attention.n.01',\n",
       "    'definition': 'the process whereby a person concentrates on some features of the environment to the (relative) exclusion of others'},\n",
       "   {'name': 'be.v.01',\n",
       "    'definition': 'have the quality of being; (copula, used with an adjective or a predicate noun)'},\n",
       "   {'name': 'beloved.n.01',\n",
       "    'definition': 'a beloved person; used as terms of endearment'},\n",
       "   {'name': 'beryllium.n.01',\n",
       "    'definition': 'a light strong brittle grey toxic bivalent metallic element'},\n",
       "   {'name': 'blunt.s.04',\n",
       "    'definition': 'devoid of any qualifications or disguise or adornment'},\n",
       "   {'name': 'clash.v.02',\n",
       "    'definition': 'be incompatible; be or come into conflict'},\n",
       "   {'name': 'combination.n.01',\n",
       "    'definition': 'a collection of things that have been combined; an assemblage of separate parts or qualities'},\n",
       "   {'name': 'concurrence.n.04',\n",
       "    'definition': 'the temporal property of two things happening at the same time'},\n",
       "   {'name': 'condemn.v.01', 'definition': 'express strong disapproval of'},\n",
       "   {'name': 'content.n.01',\n",
       "    'definition': 'everything that is included in a collection and that is held or included in something'},\n",
       "   {'name': 'context.n.01',\n",
       "    'definition': 'discourse that surrounds a language unit and helps to determine its interpretation'},\n",
       "   {'name': 'contrast.n.01',\n",
       "    'definition': 'the opposition or dissimilarity of things that are compared; ,'},\n",
       "   {'name': 'culture.n.01',\n",
       "    'definition': 'a particular society at a particular time and place'},\n",
       "   {'name': 'dad.n.01',\n",
       "    'definition': 'an informal term for a father; probably derived from baby talk'},\n",
       "   {'name': 'daze.n.01',\n",
       "    'definition': 'the feeling of distress and disbelief that you have when something bad happens accidentally'},\n",
       "   {'name': 'decision.n.02',\n",
       "    'definition': 'a position or opinion or judgment reached after consideration'},\n",
       "   {'name': 'draw.n.01',\n",
       "    'definition': 'a gully that is shallower than a ravine'},\n",
       "   {'name': 'embezzle.v.01',\n",
       "    'definition': \"appropriate (as property entrusted to one's care) fraudulently to one's own use\"},\n",
       "   {'name': 'evening.n.01',\n",
       "    'definition': 'the latter part of the day (the period of decreasing daylight from late afternoon until nightfall)'},\n",
       "   {'name': 'exploitation.n.02',\n",
       "    'definition': 'an act that exploits or victimizes someone (treats them unfairly)'},\n",
       "   {'name': 'extremist.n.01',\n",
       "    'definition': 'a person who holds extreme views'},\n",
       "   {'name': 'fictional_character.n.01',\n",
       "    'definition': 'an imaginary person represented in a work of fiction (play or film or story)'},\n",
       "   {'name': 'figure.n.01',\n",
       "    'definition': 'a diagram or picture illustrating textual material'},\n",
       "   {'name': 'guise.n.01', 'definition': 'an artful or simulated semblance'},\n",
       "   {'name': 'harmful.a.01',\n",
       "    'definition': 'causing or capable of causing harm'},\n",
       "   {'name': 'hate.n.01',\n",
       "    'definition': 'the emotion of intense dislike; a feeling of dislike so strong that it demands action'},\n",
       "   {'name': 'heroism.n.01',\n",
       "    'definition': 'the qualities of a hero or heroine; exceptional or heroic courage when facing danger (especially in battle)'},\n",
       "   {'name': 'humorous.a.01',\n",
       "    'definition': 'full of or characterized by humor'},\n",
       "   {'name': 'image.n.01', 'definition': 'an iconic mental representation'},\n",
       "   {'name': 'important.a.01', 'definition': 'of great significance or value'},\n",
       "   {'name': 'inappropriate.a.01',\n",
       "    'definition': 'not suitable for a particular occasion etc'},\n",
       "   {'name': 'innocuous.a.01',\n",
       "    'definition': 'not injurious to physical or mental health'},\n",
       "   {'name': 'intend.v.01', 'definition': 'have in mind as a purpose'},\n",
       "   {'name': 'juxtaposition.n.01',\n",
       "    'definition': 'the act of positioning close together (or side by side)'},\n",
       "   {'name': 'language.n.01',\n",
       "    'definition': 'a systematic means of communicating by the use of sounds or conventional symbols'},\n",
       "   {'name': 'likely.a.01',\n",
       "    'definition': 'has a good chance of being the case or of coming about'},\n",
       "   {'name': 'make.v.03', 'definition': 'make or cause to be or to become'},\n",
       "   {'name': 'manner.n.01',\n",
       "    'definition': 'how something is done or how it happens'},\n",
       "   {'name': 'message.n.01',\n",
       "    'definition': 'a communication (usually brief) that is written or spoken or signaled'},\n",
       "   {'name': 'negative.n.01', 'definition': 'a reply of denial'},\n",
       "   {'name': 'nostalgia.n.01', 'definition': 'longing for something past'},\n",
       "   {'name': 'political_orientation.n.01',\n",
       "    'definition': 'an orientation that characterizes the thinking of a group or nation'},\n",
       "   {'name': 'promote.v.01',\n",
       "    'definition': 'contribute to the progress or growth of'},\n",
       "   {'name': 'show.v.01',\n",
       "    'definition': 'give an exhibition of to an interested audience'},\n",
       "   {'name': 'spread.n.01',\n",
       "    'definition': 'process or result of distributing or extending over a wide expanse of space'},\n",
       "   {'name': 'strength.n.01',\n",
       "    'definition': 'the property of being physically or mentally strong'},\n",
       "   {'name': 'stud.n.01',\n",
       "    'definition': 'a man who is virile and sexually active'},\n",
       "   {'name': 'such.s.01', 'definition': 'of so extreme a degree or extent'},\n",
       "   {'name': 'symbolizing.n.01',\n",
       "    'definition': 'the act of representing something with a symbol'},\n",
       "   {'name': 'term.n.01',\n",
       "    'definition': 'a word or expression used for some particular thing'},\n",
       "   {'name': 'type.n.01',\n",
       "    'definition': 'a subdivision of a particular kind of thing'},\n",
       "   {'name': 'use.n.01', 'definition': 'the act of using'},\n",
       "   {'name': 'violence.n.01',\n",
       "    'definition': 'an act of aggression (as one against a person who resists)'},\n",
       "   {'name': 'well-known.s.01', 'definition': 'widely or fully known'},\n",
       "   {'name': 'wit.n.01',\n",
       "    'definition': 'a message whose ingenuity or verbal skill or incongruity has the power to evoke laughter'}]},\n",
       " 'extracted_ne_qids': {'visual': [{'qid': 'Q102317472',\n",
       "    'name': 'Grayskull wird verschleppt'},\n",
       "   {'qid': 'Q102317472', 'name': 'The Taking of Grayskull'},\n",
       "   {'qid': 'Q114550105', 'name': 'The Power of Grayskull'},\n",
       "   {'qid': 'Q114550105', 'name': 'Grayskulls styrka'},\n",
       "   {'qid': 'Q125259066', 'name': 'Castle Grayskull'},\n",
       "   {'qid': 'Q2479649', 'name': 'Grayskull'},\n",
       "   {'qid': 'Q2479649', 'name': 'Castle Grayskull'},\n",
       "   {'qid': 'Q3965837', 'name': 'Grayskullsvärdet'},\n",
       "   {'qid': 'Q3965837', 'name': 'Power Sword'}],\n",
       "  'textual': [{'qid': 'Q126096643', 'name': 'neo-Nazi'},\n",
       "   {'qid': 'Q151250', 'name': 'neo-Nazism'},\n",
       "   {'qid': 'Q15994152', 'name': 'Racial Holy War'},\n",
       "   {'qid': 'Q1819273', 'name': 'Ben Klassen'},\n",
       "   {'qid': 'Q1819273', 'name': 'Racial Holy War'},\n",
       "   {'qid': 'Q2625151', 'name': 'white power skinhead'},\n",
       "   {'qid': 'Q2625151', 'name': 'neo-Nazi skinhead'},\n",
       "   {'qid': 'Q4400801', 'name': 'Neo-Nazism in Russia'},\n",
       "   {'qid': 'Q533914', 'name': 'neo-Nazi black metal'},\n",
       "   {'qid': 'Q533914', 'name': 'National Socialist black metal'},\n",
       "   {'qid': 'Q63214670',\n",
       "    'name': \"Neo-Nazi banned from white supremacist websites for calling Jews 'vermin' and 'parasites' is allowed back online - to prove his racist comments are true\"},\n",
       "   {'qid': 'Q65158223', 'name': 'Neo-Nazism in Ukraine'},\n",
       "   {'qid': 'Q679584', 'name': 'Racial holy war'},\n",
       "   {'qid': 'Q679584', 'name': 'Creativity'},\n",
       "   {'qid': 'Q77977913', 'name': 'Racial Holy War: The Cold War'}],\n",
       "  'combined': []}}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linked_ontox_dict[\"chatgpt_2b.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d24b9-412b-401a-bab2-602c1f9e1d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
