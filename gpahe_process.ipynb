{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7beda2b5-3c6b-43b7-9465-7bc753976b30",
   "metadata": {},
   "source": [
    "# Process GPAHE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0631237a-407e-424c-8173-66ab2922ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from IPython.display import Image\n",
    "from wikidata.client import Client\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c37271-6d39-47cf-bb8a-5febbfad283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"../../data/scraped_symbol_dict.json\"\n",
    "data_df = pd.read_json(DATA_FILE).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868c8491-144f-4a9a-8030-7e9632efa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: \n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9b582-5c8e-428f-9ec3-1c84a0aeebf3",
   "metadata": {},
   "source": [
    "## 1. Cluster hate symbols\n",
    "\n",
    "Use https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68bd9c4-8afb-4179-a798-0cc455175801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import regex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a9d7c-215b-4ae7-a49a-ceb3e776e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_data(data_df):\n",
    "    texts = []\n",
    "    for _, row in data_df.iterrows():\n",
    "        description = row[\"Description\"]\n",
    "        ideology = \". ideology is \" + regex.sub(\",\", \" . ideology is\", row[\"Ideology\"])\n",
    "        location = \". location is \" + regex.sub(\",\", \" . location is\", row[\"Location\"])\n",
    "        texts.append(\" \".join(word_tokenize(\" \".join([description, ideology, location]))))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c55a1-2b12-41b9-90f0-c3c5dfb5992f",
   "metadata": {},
   "source": [
    "Interesting words for clustering:\n",
    "* chapter\n",
    "* club/klub\n",
    "* group\n",
    "* organization\n",
    "* party\n",
    "* proud (boys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6067fc7-f7a9-4895-b3ad-d8a41fb28e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text_data(texts, nbr_of_dimensions=5):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    vectorized_data = TruncatedSVD(n_components=nbr_of_dimensions, n_iter=5, random_state=42)\n",
    "    vectorized_data.fit(X.T)\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9636a-cdc3-4245-9046-623df8a415c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(texts, vectorized_data, labels, dimension_1=0, dimension_2=1):\n",
    "    try:\n",
    "        x = vectorized_data.components_[dimension_1]\n",
    "        y = vectorized_data.components_[dimension_2]\n",
    "    except:\n",
    "        raise ValueError(f\"invalid pair of dimensions ({dimension_1}, {dimension_2})\")\n",
    "    split_data, labels = split_data_by_content(x, y, texts, labels)\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    for color in sorted(split_data.keys(), reverse=True):\n",
    "        plt.scatter(split_data[color][0], split_data[color][1], c=color, label=color, alpha=0.5)\n",
    "    for i in range(0, len(x)):\n",
    "        plt.annotate(str(i), (x[i], y[i]))\n",
    "    plt.legend(labels=[ label for color, label in sorted(labels.items(), reverse=True) ] )\n",
    "    plt.title(f\"{len(x)} hate symbols clustered by description, ideology and location\")\n",
    "    plt.savefig(\"gpahe_process.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf9523-88e5-4309-8fa8-0dd1cbff8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_token = \"proud boys\"\n",
    "orange_token = \"chapter\"\n",
    "yellow_token = \"club\"\n",
    "green_token = \"group\"\n",
    "other_color = \"blue\"\n",
    "labels = { \"yellow\": yellow_token, \"red\": red_token, \"orange\": orange_token, \"green\": green_token, other_color: \"other\" }\n",
    "\n",
    "def split_data_by_content(x, y, texts, labels):\n",
    "    split_data = { other_color: [[], []], \"green\": [[], []], \"orange\": [[], []], \"red\": [[], []], \"yellow\": [[], []] }\n",
    "    for i in range(0, len(x)):\n",
    "        if regex.search(red_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"red\"][0].append(x[i])\n",
    "            split_data[\"red\"][1].append(y[i])\n",
    "        elif regex.search(orange_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"orange\"][0].append(x[i])\n",
    "            split_data[\"orange\"][1].append(y[i])\n",
    "        elif regex.search(yellow_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"yellow\"][0].append(x[i])\n",
    "            split_data[\"yellow\"][1].append(y[i])\n",
    "        elif regex.search(green_token, texts[i], regex.IGNORECASE):\n",
    "            split_data[\"green\"][0].append(x[i])\n",
    "            split_data[\"green\"][1].append(y[i])\n",
    "        else:\n",
    "            split_data[other_color][0].append(x[i])\n",
    "            split_data[other_color][1].append(y[i])\n",
    "    for color in labels:\n",
    "        labels[color] += f\" ({len(split_data[color][0])})\"\n",
    "    return split_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8a54c-e85d-429e-9957-3813e363a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_text_data(data_df)\n",
    "vectorized_data = vectorize_text_data(texts)\n",
    "visualize(texts, vectorized_data, labels.copy(), dimension_1=1, dimension_2=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4efd3d-b836-4500-9502-6be3bdb9d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.iloc[422][\"Description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975b3cd-85b7-4504-8b7c-b4b189a12fdc",
   "metadata": {},
   "source": [
    "## 2. Make a knowledge base\n",
    "\n",
    "* https://medium.com/nlplanet/building-a-knowledge-base-from-texts-a-full-practical-example-8dbbffb912fa\n",
    "* https://neo4j.com/blog/text-to-knowledge-graph-information-extraction-pipeline/\n",
    "* https://neo4j.com/developer-blog/construct-knowledge-graphs-unstructured-text/\n",
    "* https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a449e-e629-4ee8-b02c-c6b2d1a67cab",
   "metadata": {},
   "source": [
    "## 3. Extract entities with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fd6be-43fc-4f22-865f-ebeb26c42ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_FIELD_NAMES = \"CARDINAL DATE EVENT FAC GPE LANGUAGE LAW LOC MONEY NOMINAL NORP ORDINAL ORG PERCENT PERSON PRODUCT QUANTITY TIME WORK_OF_ART\".split()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba3d19-355f-436c-bd86-a1f929a68f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_values = { \"organizations\": [\"NORP\", \"ORG\" ],\n",
    "                 \"locations\": [ \"FAC\", \"GPE\", \"LOC\", ], \n",
    "                 \"events\": [ \"EVENT\", ],\n",
    "                 \"dates\": [ \"DATE\", \"TIME\", ], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6dbe2a-0378-47c1-adc8-9f0a41698aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(nlp_analysis):\n",
    "    entities = {}\n",
    "    for entity in nlp_analysis.ents:\n",
    "        if entity.label_ not in entities:\n",
    "            entities[entity.label_] = []\n",
    "        entities[entity.label_].append(entity.text)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15c641-f955-4118-ad8d-8dd5c350ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_analysis(texts, nlp):\n",
    "    entity_data = []\n",
    "    for text in texts:\n",
    "        nlp_analysis = nlp(text) \n",
    "        entities = get_entities(nlp_analysis)\n",
    "        entity_data.append(entities)\n",
    "    return entity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b2e82-d2ca-4077-a248-af43ac9a3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entity_groups(entity_data, entity_group_name, n=10):\n",
    "    values = {}\n",
    "    for entities in entity_data:\n",
    "        for entity_name in entities:\n",
    "            if entity_name in field_values[entity_group_name]:\n",
    "                for entity in entities[entity_name]:\n",
    "                    if entity in values:\n",
    "                        values[entity] += 1\n",
    "                    else:\n",
    "                        values[entity] = 1\n",
    "    print(f\"{entity_group_name} ({sum(values.values())}):\", [[key, value] for key, value in sorted(values.items(), key=lambda item: item[1], reverse=True)][:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5b9b4-c52e-4c2b-9463-9f83bf5575cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entities(entity_data, target_entity_name, n=10):\n",
    "    values = {}\n",
    "    for entities in entity_data:\n",
    "        for entity_name in entities:\n",
    "            if entity_name == target_entity_name:\n",
    "                for entity in entities[entity_name]:\n",
    "                    if entity in values:\n",
    "                        values[entity] += 1\n",
    "                    else:\n",
    "                        values[entity] = 1\n",
    "    print(f\"{target_entity_name} ({sum(values.values())}):\", [[key, value] for key, value in sorted(values.items(), key=lambda item: item[1], reverse=True)][:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bf4d1-1c22-456c-b63e-fbc041a8f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data = nlp_analysis(texts, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dddc96-bf7c-4016-b316-7eed2ee8d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field_name in SPACY_FIELD_NAMES:\n",
    "    count_entities(entity_data, field_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35f45c-f39e-4a49-adde-43ccfcf469a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"organizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcd6fc-8f02-4571-90e7-1301ab080712",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de616c-e236-4aca-879d-c63636ea780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e35ad4-dc31-4cd3-8a6d-56e9882b55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_entity_groups(entity_data, \"dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31707462-0f3d-40e1-b159-74dc18e11611",
   "metadata": {},
   "source": [
    "## 4. Extract entities with REBEL\n",
    "\n",
    "Based on blog by Fabio Chiusano: https://medium.com/nlplanet/building-a-knowledge-base-from-texts-a-full-practical-example-8dbbffb912fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90fa06-3867-47a9-98b3-28ef5c9bdf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "import wikipedia\n",
    "from newspaper import Article, ArticleException\n",
    "from GoogleNews import GoogleNews\n",
    "import IPython\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe418b8-b76c-4517-a636-4becbefa0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976491bc-7807-489c-ba1c-21940fd8f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        relations.append({\n",
    "            'head': subject.strip(),\n",
    "            'type': relation.strip(),\n",
    "            'tail': object_.strip()\n",
    "        })\n",
    "    return relations\n",
    "\n",
    "# source: https://gist.githubusercontent.com/fabiochiusano/934ad5ff318626befbdd20c72e074186/raw/e3e44110a0db5408d17fba52be559ecaf676b6d2/kb_4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa85990-f73e-44bc-a7d4-31d1d83e07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB():\n",
    "    def __init__(self):\n",
    "        self.relations = []\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "    def add_relation(self, r):\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")\n",
    "\n",
    "# source: https://gist.githubusercontent.com/fabiochiusano/e64d5250371e18f7a6cc02ac0cdc64c5/raw/24af0f7f23b313591fe91fc9f8826cf216ca4568/kb_5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4edd6d-e03c-494f-992f-b5b5288f5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_small_text_to_kb(text, verbose=False):\n",
    "    kb = KB()\n",
    "\n",
    "    # Tokenizer text\n",
    "    model_inputs = tokenizer(text, max_length=512, padding=True, truncation=True,\n",
    "                            return_tensors='pt')\n",
    "    if verbose:\n",
    "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
    "\n",
    "    # Generate\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 216,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": 3\n",
    "    }\n",
    "    generated_tokens = model.generate(\n",
    "        **model_inputs,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "    # create kb\n",
    "    for sentence_pred in decoded_preds:\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for r in relations:\n",
    "            kb.add_relation(r)\n",
    "\n",
    "    return kb\n",
    "\n",
    "# source: https://gist.githubusercontent.com/fabiochiusano/ceec4d9ff1ce2ad25c40fbd8412aa9e4/raw/796771f88776fca9d7c4c84bd1b3a52d9ef5b5c1/kb_6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de33da-7d9c-41e1-a0dc-cdc60890cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_per_text(texts):\n",
    "    relations_per_text = []\n",
    "    for text in texts:\n",
    "        relations = []\n",
    "        for sentence in sent_tokenize(text):\n",
    "            kb = from_small_text_to_kb(sentence, verbose=True)\n",
    "            relations.extend(kb.__dict__[\"relations\"])\n",
    "        relations_per_text.append(relations)\n",
    "        squeal(f\"{len(relations_per_text)}: {sum([len(relations) for relations in relations_per_text])/len(relations_per_text)}\")\n",
    "    return relations_per_text\n",
    "\n",
    "# source for line 6: https://gist.githubusercontent.com/fabiochiusano/a720da218ee8d19de3130fa36c23a69b/raw/a9b94a3ddbad61cfb3713234476423fffbfdca41/kb_7.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61952a3-931c-44ee-ad64-d67f7eb4e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_relations(relations_per_text):\n",
    "    relations_count = {}\n",
    "    for relations in relations_per_text:\n",
    "        for relation in relations:\n",
    "            key = \"#\".join([relation[\"head\"], relation[\"type\"], relation[\"tail\"]])\n",
    "            if key in relations_count:\n",
    "                relations_count[key] += 1\n",
    "            else:\n",
    "                relations_count[key] = 1\n",
    "    return relations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc33865-41da-45fb-9400-28384d861fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relations_to_df(relations_per_text):\n",
    "    relations_list = []\n",
    "    for counter in range(0, len(relations_per_text)):\n",
    "        for relation in relations_per_text[counter]:\n",
    "            relation[\"text_id\"] = counter\n",
    "            relations_list.append(relation)\n",
    "    relations_df = pd.DataFrame(relations_list)\n",
    "    return relations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8237c-c974-444f-84c3-35c5fe14a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_of_lists(dict_of_lists):\n",
    "    return [ (len(list), key) for key, list in sorted(dict_of_lists.items(), key=lambda x: len(x[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd6932-9815-4421-88a9-ea327df2ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicates(relations_df):\n",
    "    nbr_of_duplicates = 0\n",
    "    seen = {}\n",
    "    for index, relation in relations_df.iterrows():\n",
    "        key = \"#\".join([relation[\"head\"], relation[\"type\"], relation[\"type\"]])\n",
    "        if key in seen:\n",
    "            nbr_of_duplicates += 1\n",
    "        seen[key] = True\n",
    "    return nbr_of_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a8205-1dc3-46b1-afb2-cc8c243a6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_relation_fields(relations_df):\n",
    "    head_terms = {}\n",
    "    tail_terms = {}\n",
    "    type_head_terms = {}\n",
    "    type_tail_terms = {}\n",
    "\n",
    "    for index, relation in relations_df.iterrows():\n",
    "        head = relation[\"head\"]\n",
    "        tail = relation[\"tail\"]\n",
    "        type = relation[\"type\"]\n",
    "        if head not in head_terms:\n",
    "            head_terms[head] = []\n",
    "        if tail not in head_terms[head]:\n",
    "            head_terms[head].append(tail)\n",
    "        if tail not in tail_terms:\n",
    "            tail_terms[tail] = []\n",
    "        if head not in tail_terms[tail]:\n",
    "            tail_terms[tail].append(head)\n",
    "        key = f\"{type}({head},_)\"\n",
    "        if key not in type_head_terms:\n",
    "            type_head_terms[key] = []\n",
    "        if tail not in type_head_terms[key]:\n",
    "            type_head_terms[key].append(tail)\n",
    "        key = f\"{type}(_,{tail})\"\n",
    "        if key not in type_tail_terms:\n",
    "            type_tail_terms[key] = []\n",
    "        if head not in type_tail_terms[key]:\n",
    "            type_tail_terms[key].append(head)\n",
    "    return head_terms, tail_terms, type_head_terms, type_tail_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98fbe3-b1d9-48d4-878b-190d2106103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_per_text = extract_relations_per_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0751015-edda-4152-8b67-ffbe03b5e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_count = count_relations(relations_per_text)\n",
    "print(f\"number of relations: {sum(relations_count.values())}; unique: {len(relations_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5f101-2f5f-4fd2-8110-9780c77a059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_freqs(relations_count)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe522989-9689-4af2-9d5a-805c3efb8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df = convert_relations_to_df(relations_per_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffcda8-8bab-459f-bfa8-c1c5307c1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df.to_csv(\"gpahe_process.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee6172-a257-4967-84a5-2dfa8d0a6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_terms, tail_terms, type_head_terms, type_tail_terms = count_relation_fields(relations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583abde0-22c5-454e-8bf9-cc56a5cee7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(head_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef68304-e25f-471a-bd36-5e35b05720f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(tail_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0abac8-f7d6-4ea7-a1fe-1751b0d6c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(type_head_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f5d5f-3fe5-4d68-ab2b-d6eb86f192eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict_of_lists(type_tail_terms)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5b912-fa54-42a1-a5ea-5c6eb2dbd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f210e0-f9b4-486c-804b-4230c4172e27",
   "metadata": {},
   "source": [
    "## 5. Visualize knowlegde triples\n",
    "\n",
    "To do: add labels to edges (note: an edge can have several labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda225b-09cc-4dec-a436-17f6f74b4b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec5459-7768-45dd-af33-a8509804465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(relations_df, source=\"head\", target=\"tail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d803385-77d0-4d91-9424-7564bcbe812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(notebook=True)\n",
    "net.from_nx(G)\n",
    "net.show(\"gpahe_process.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606e82b-0198-48d0-a33d-5c90f2c33a24",
   "metadata": {},
   "source": [
    "## 6. Link to wikidata (fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787b9b8-8346-4e20-9483-9b028bba6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()  # doctest: +SKIP\n",
    "entity = client.get('Q20145', load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eec733-ee86-4966-b77c-fbd2fe1c9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f79870-0780-41f6-9ba1-6f35dc4214ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "from pywikibot import pagegenerators, WikidataBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b10966-6ae2-410e-9755-a55dbc6e742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = \"SELECT ?item WHERE { ?item rdfs:label 'Google'@en }\"\n",
    "entities = pagegenerators.WikidataSPARQLPageGenerator(sparql)\n",
    "entities = list(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa72825-6b52-486e-8879-e269ad83e7ff",
   "metadata": {},
   "source": [
    "## 7. download logo's manually (lot of work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f3e21-55f6-44e3-adeb-638b669f0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ffa22-a21a-45e7-addb-751c8171a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://symbols.globalextremism.org/details?recordId=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b558a6-8dd1-4e6d-a2b3-4f641c712899",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTER_MINIMUM = 80\n",
    "counter = 0\n",
    "for id in data_df.index:\n",
    "    counter += 1\n",
    "    if counter > COUNTER_MINIMUM:\n",
    "       webbrowser.open(base_url + id, new=2)\n",
    "       print(counter)\n",
    "       input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3467e9-0196-46ce-84bc-2d54e2953ee4",
   "metadata": {},
   "source": [
    "1. open the web page in a new tab of a browser\n",
    "2. right click on the logo/image\n",
    "3. open the image in a new tab of the browser\n",
    "4. right click on the image\n",
    "5. save the image with name number.extension\n",
    "6. close the two added tabs\n",
    "7. push the enter/return button on the notebook page\n",
    "8. repeat for the next logo/image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e1e48d-94ac-473c-a067-a1ff49a0d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = os.environ[\"HOME\"] + \"/Downloads\"\n",
    "download_files = os.listdir(download_dir)\n",
    "extensions = [ \"jpg\", \"png\", \"svg\", \"webp\", \"JPG\", \"PNG\", \"SVG\", \"WEBP\" ]  \n",
    "counter = 0\n",
    "for id in data_df.index:\n",
    "    counter += 1\n",
    "    if counter > COUNTER_MINIMUM:\n",
    "        break\n",
    "    file_found = False\n",
    "    for extension in extensions:\n",
    "        if os.path.isfile(download_dir + \"/\" + str(counter) + \".\" + extension):\n",
    "            file_found = True\n",
    "            break\n",
    "    if not file_found:\n",
    "        print(f\"cannot find file number {counter}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d243e-23ae-4e53-935a-69502938068f",
   "metadata": {},
   "source": [
    "## 8. Link Spacy entities from ChatGPT output to GPAHE metedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c85de4-b840-4cbd-b6be-c15e71af29c0",
   "metadata": {},
   "source": [
    "From the Spacy analysis we select all:\n",
    "1. noun phrases\n",
    "2. entities\n",
    "3. tokens with pos tag PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5896098b-7200-4715-bde9-03a1217d8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c13708-2209-4376-8019-f480b2ad18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_chars = str.maketrans(\"*#\", \"  \")\n",
    "\n",
    "def get_phrases(text, spacy_model):\n",
    "    nlp_analysis = spacy_model(text.translate(remove_chars))\n",
    "    chunk_texts = [ regex.sub(\"^[Tt][Hh][EeIi][Ss]* \", \"\", \n",
    "                        regex.sub(\"^[Aa][Nn]* \", \"\", chunk.text, \n",
    "                                  regex.IGNORECASE), \n",
    "                              regex.IGNORECASE) \n",
    "                     for chunk in nlp_analysis.noun_chunks\n",
    "                  ]\n",
    "    chunk_texts.extend([entity.text for entity in nlp_analysis.ents])\n",
    "    chunk_texts.extend([token.text for token in nlp_analysis if token.pos_ == \"PROPN\" ])\n",
    "    return chunk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a4aced-22ea-423c-b5c6-c9e86998f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_list(data_df):\n",
    "    term_dict = {}\n",
    "    for index, row in data_df.iterrows():\n",
    "        term_dict[row[\"Title\"]] = True\n",
    "        for ideology in row[\"Ideology\"].split(\",\"):\n",
    "            term_dict[ideology.strip()] = True\n",
    "        for location in row[\"Location\"].split(\",\"):\n",
    "            term_dict[location.strip()] = True\n",
    "    return sorted(term_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee8f1d6-58b1-4b3c-884d-798561d90133",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_dir = \"chatgpt\"\n",
    "\n",
    "def read_chatgpt_texts(chatgpt_dir):\n",
    "    chatgpt_files = [ file_name for file_name in os.listdir(chatgpt_dir)\n",
    "                      if regex.search(\"b.txt\", file_name) ]\n",
    "    chatgpt_texts = {}\n",
    "    for file_name in sorted(chatgpt_files):\n",
    "        file_handle = open(os.path.join(chatgpt_dir, file_name), \"r\")\n",
    "        lines = file_handle.readlines()\n",
    "        file_handle.close()\n",
    "        chatgpt_texts[\"_\".join([chatgpt_dir, file_name])] = \" \".join(lines)\n",
    "    return chatgpt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f988ede-1492-4f31-9d92-13a2826ea64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meme_text_from_chatgpt_text(text):\n",
    "    meme_text = \"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if regex.search(\"\\\"\", line):\n",
    "            line = regex.sub('\" and \"', r\"\\n\", line)\n",
    "            line = regex.sub('^[^\"]*\"', \"\", line)\n",
    "            line = regex.sub(\"\\\".*$\", \"\", line)\n",
    "            meme_text = line\n",
    "            break\n",
    "    return meme_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0257e06-3b44-4cac-a127-3635c792caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "term_list = get_term_list(data_df)\n",
    "chatgpt_texts = read_chatgpt_texts(chatgpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51d0e06e-4f79-4aa8-87a4-37c101ab0a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4/20']\n",
      "['neo-Nazi']\n",
      "['Pepe the Frog']\n",
      "['Nazi', 'Germany']\n"
     ]
    }
   ],
   "source": [
    "term_list_lower = [ term.lower() for term in term_list ]\n",
    "meme_texts = {}\n",
    "for file_name in sorted(chatgpt_texts.keys()):\n",
    "    phrases = list(set(get_phrases(chatgpt_texts[file_name], spacy_model)))\n",
    "    phrases_in_term_list = [ phrase for phrase in phrases if phrase.lower() in term_list_lower ]\n",
    "    meme_texts[\"_\".join([chatgpt_dir, file_name])] = get_meme_text_from_chatgpt_text(chatgpt_texts[file_name])\n",
    "    print(phrases_in_term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9251296f-8374-4db0-9096-5af061db1af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 1. Interpretation of the Image\\n The image shows a dog with a happy, slightly mischievous expression. The background is decorated with colorful, psychedelic patterns that resemble marijuana leaves, often associated with a state of altered consciousness or celebration.\\n \\n ### 2. Interpretation of the Text\\n The text reads: \"ITS GONNA BE 4/20 FOR A WHOLE MONTH.\" This is a play on the date April 20th (4/20), which is widely recognized in cannabis culture as a day for celebrating and consuming marijuana.\\n \\n ### 3. Interpretation of the Combination\\n The combination of the happy, relaxed dog and the text implies a humorous and exaggerated scenario where the state of celebration and relaxation associated with 4/20 lasts for an entire month. The dog\\'s expression, along with the colorful background, reinforces the playful and light-hearted tone of the message, suggesting an extended period of enjoyment and leisure.\\n \\n In essence, the meme is using humor to exaggerate the idea of an extended celebration of cannabis culture, combining visual and textual elements to create a playful, humorous effect.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_texts[\"chatgpt_1b.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bcf3b2-e387-4c85-834c-e95066d420c0",
   "metadata": {},
   "source": [
    "## 9. Get WordNet synsets from ChatGPT texts\n",
    "\n",
    "Uses ``get_meme_text_from_chatgpt_text`` from code block 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "225b42d7-193e-4bc5-b8e9-7fccdd94aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from rdflib import Graph\n",
    "from nltk import word_tokenize, pos_tag, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e310f47b-c889-4f6c-84c2-f494b670455f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N29f7774bf4b64c3eb105e674e1485531 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse(\"../data/ontox_kg.ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "712c57b4-3b8a-4155-8dd2-2c0bbaeff6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [ \"visual\", \"textual\", \"combined\" ]\n",
    "\n",
    "def split_text(chatgpt_text):\n",
    "    chatgpt_texts_split = { mode: \"\" for mode in modes }\n",
    "    mode = 0\n",
    "    for line in chatgpt_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line == \"### 1. Interpretation of the Image\":\n",
    "            mode = 0\n",
    "        elif line == \"### 2. Interpretation of the Text\":\n",
    "            mode = 1\n",
    "        elif line == \"### 3. Interpretation of the Combination\":\n",
    "            mode = 2\n",
    "        else:\n",
    "            chatgpt_texts_split[modes[mode]] += \"\\n\" + line\n",
    "    return chatgpt_texts_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "841f0023-1004-45db-a740-0ec7d3d3782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_info(entity_label):\n",
    "    \"\"\"\n",
    "    Function to query Wikidata API for a given entity label and return its QID and name.\n",
    "\n",
    "    Args:\n",
    "    - `entity_label` (str): Label of the entity to be queried.\n",
    "    \n",
    "    Returns:\n",
    "    - `str`: QID of the entity.\n",
    "    - `str`: Name of the entity.\n",
    "    \n",
    "    Dependencies:\n",
    "    - `requests`: For querying Wikidata API.\n",
    "    \n",
    "    Output:\n",
    "    - Returns the QID and name of the entity if found, otherwise returns `None`.\n",
    "    \n",
    "    \"\"\"\n",
    "    url = f\"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"en\",\n",
    "        \"limit\": 100,\n",
    "        \"uselang\": \"en\",\n",
    "        \"search\": entity_label\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    if 'search' in data.keys():\n",
    "        return data[\"search\"]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b65441f5-7798-46e7-a860-bdc21925f02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Q15994152',\n",
       "  'title': 'Q15994152',\n",
       "  'pageid': 17611956,\n",
       "  'concepturi': 'http://www.wikidata.org/entity/Q15994152',\n",
       "  'repository': 'wikidata',\n",
       "  'url': '//www.wikidata.org/wiki/Q15994152',\n",
       "  'display': {'label': {'value': 'Racial Holy War', 'language': 'en'},\n",
       "   'description': {'value': 'White Supremacist concept', 'language': 'en'}},\n",
       "  'label': 'Racial Holy War',\n",
       "  'description': 'White Supremacist concept',\n",
       "  'match': {'type': 'label', 'language': 'en', 'text': 'Racial Holy War'}},\n",
       " {'id': 'Q77977913',\n",
       "  'title': 'Q77977913',\n",
       "  'pageid': 77436094,\n",
       "  'concepturi': 'http://www.wikidata.org/entity/Q77977913',\n",
       "  'repository': 'wikidata',\n",
       "  'url': '//www.wikidata.org/wiki/Q77977913',\n",
       "  'display': {'label': {'value': 'Racial Holy War: The Cold War',\n",
       "    'language': 'en'},\n",
       "   'description': {'value': '2016 first-person shooter video game',\n",
       "    'language': 'en'}},\n",
       "  'label': 'Racial Holy War: The Cold War',\n",
       "  'description': '2016 first-person shooter video game',\n",
       "  'match': {'type': 'label',\n",
       "   'language': 'en',\n",
       "   'text': 'Racial Holy War: The Cold War'}},\n",
       " {'id': 'Q1819273',\n",
       "  'title': 'Q1819273',\n",
       "  'pageid': 1750135,\n",
       "  'concepturi': 'http://www.wikidata.org/entity/Q1819273',\n",
       "  'repository': 'wikidata',\n",
       "  'url': '//www.wikidata.org/wiki/Q1819273',\n",
       "  'display': {'label': {'value': 'Ben Klassen', 'language': 'en'},\n",
       "   'description': {'value': 'American engineer, author and politician (1918-1993)',\n",
       "    'language': 'en'}},\n",
       "  'label': 'Ben Klassen',\n",
       "  'description': 'American engineer, author and politician (1918-1993)',\n",
       "  'match': {'type': 'label', 'language': 'sv', 'text': 'Racial Holy War'},\n",
       "  'aliases': ['Racial Holy War']},\n",
       " {'id': 'Q679584',\n",
       "  'title': 'Q679584',\n",
       "  'pageid': 640206,\n",
       "  'concepturi': 'http://www.wikidata.org/entity/Q679584',\n",
       "  'repository': 'wikidata',\n",
       "  'url': '//www.wikidata.org/wiki/Q679584',\n",
       "  'display': {'label': {'value': 'Creativity', 'language': 'en'},\n",
       "   'description': {'value': 'pantheistic white separatist religious movement, founded in Lighthouse Point, Florida by Ben Klassen in 1973; promotes the veneration of the white race and the safeguarding of its survival',\n",
       "    'language': 'en'}},\n",
       "  'label': 'Creativity',\n",
       "  'description': 'pantheistic white separatist religious movement, founded in Lighthouse Point, Florida by Ben Klassen in 1973; promotes the veneration of the white race and the safeguarding of its survival',\n",
       "  'match': {'type': 'alias', 'language': 'fi', 'text': 'Racial holy war'},\n",
       "  'aliases': ['Racial holy war']}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wikidata_info(\"Racial Holy War\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461f954-56b4-4c47-bc1c-38603c411cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entities(paragraph, spacy_model):\n",
    "    doc = spacy_model(paragraph)\n",
    "    entity_info = []\n",
    "    for entity_text in set([entity.text for entity in doc.ents]):\n",
    "        wikidata_info = get_wikidata_info(entity_text)\n",
    "        for wikidata_item in wikidata_info:\n",
    "            entity_info.append((wikidata_item[\"id\"], wikidata_item[\"label\"]))\n",
    "            if \"aliases\" in wikidata_item:\n",
    "                print(wikidata_item[\"label\"], wikidata_item[\"aliases\"])\n",
    "                for label in wikidata_item[\"aliases\"]:\n",
    "                    entity_info.append((wikidata_item[\"id\"], label))\n",
    "    return sorted(set(entity_info), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba1fa2-0ea8-45f9-ad50-b1e849cf42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_synsets(paragraph):\n",
    "    tokens = word_tokenize(paragraph)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    relevant_synsets = set()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token, tag in tagged_tokens:\n",
    "        if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ') or tag.startswith('RB'):\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            synsets = wn.synsets(lemma)\n",
    "            if synsets:\n",
    "                synset = synsets[0]\n",
    "                relevant_synsets.add((synset.name(), synset.definition()))\n",
    "    return sorted(list(relevant_synsets), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbf767-f88b-4a41-9dcb-4d0f30672141",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontox_dict = json.load(open('../../data/ontox_dict.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e341e-a678-40d8-ad20-90b50d428832",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_ontox_dict = {}\n",
    "for file_name in chatgpt_texts:\n",
    "    chatgpt_text_split = split_text(chatgpt_texts[file_name])\n",
    "    meme_text = get_meme_text_from_chatgpt_text(chatgpt_text_split[\"textual\"])\n",
    "    linked_ontox_dict[file_name] = {\n",
    "        \"Image_URL\": \"unknown\",\n",
    "        \"Meme_text\": meme_text,\n",
    "        \"Visual_description\": chatgpt_text_split[\"visual\"],\n",
    "        \"Textual_description\": chatgpt_text_split[\"textual\"],\n",
    "        \"Combined_description\": chatgpt_text_split[\"combined\"],\n",
    "        \"extracted_synsets\": {},\n",
    "        \"extracted_ne_qids\": {}\n",
    "    }\n",
    "    for mode in modes:\n",
    "        synsets = extract_synsets(chatgpt_text_split[mode])\n",
    "        named_entities = get_named_entities(chatgpt_text_split[mode], spacy_model)\n",
    "        linked_ontox_dict[file_name][\"extracted_synsets\"][mode] = [{\"name\": synset[0], \"definition\": synset[1]} for synset in synsets]\n",
    "        linked_ontox_dict[file_name][\"extracted_ne_qids\"][mode] = [{\"qid\": entity[0], \"name\": entity[1]} for entity in named_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c58fd3-f301-4428-8331-807baaa1246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_handle = open(\"gpahe_process.json\", \"w\")\n",
    "json.dump(linked_ontox_dict, outfile_handle)\n",
    "outfile_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68476aef-a07e-435c-a7e3-bfdeefd1f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_ontox_dict[\"chatgpt_2b.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d24b9-412b-401a-bab2-602c1f9e1d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
